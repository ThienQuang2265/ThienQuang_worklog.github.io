---
title: "Week 5 Worklog"
date: "2025-10-06"
weight: 5
chapter: false
pre: " <b> 1.5. </b> "
---

**Week Duration: October 6 - October 12, 2025**

### Week 5 Objectives:

- Test ML models to determine which architecture best fits the dataset (XGBoost vs Neural Network)  
- Understand why XGBoost consistently excels for tabular data  
- Learn NoSQL and DynamoDB fundamentals and how they support scalable applications  
- Build a baseline XGBoost model for the final project  

### Tasks to be carried out this week:

| Day         | Task                                                                                                                                                                                                                  | Start Date | Completion Date | Reference Material                                          |
|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|------------------|-------------------------------------------------------------|
| Monday         | - **Model Architecture Testing:** <br>&emsp; + Compare XGBoost and Neural Networks <br>&emsp; + Evaluate training time, overfitting behavior, and validation performance                                             | 10/06/2025 | 10/06/2025       | ML Model Comparison Notes                                   |
| Tuesday         | - **Why XGBoost Excels:** <br>&emsp; + Study tree-based advantages <br>&emsp; + Read research on XGBoost vs deep learning for tabular data                                                                          | 10/07/2025 | 10/07/2025       | https://arxiv.org/pdf/2207.08815                            |
| Wednesday-Thursday         | - **DynamoDB & NoSQL Discovery:** <br>&emsp; + Learn NoSQL fundamentals <br>&emsp; + Understand DynamoDB's scalability model <br>&emsp; + Learn partition keys, sort keys, RCU/WCU, and autoscaling strategies      | 10/08/2025 | 10/09/2025       | https://www.youtube.com/watch?v=0buKQHokLK8                 |
| Friday          | - **Baseline XGBoost Model:** <br>&emsp; + Build baseline XGBoost model <br>&emsp; + Perform hyperparameter tuning <br>&emsp; + Document metrics for later model comparison                                         | 10/10/2025 | 10/10/2025       | XGBoost Documentation                                       |

### Week 5 Achievements:

- **Model Comparison Completed:**
  - Tested XGBoost vs Neural Network performance  
  - Identified the best architecture for the dataset based on accuracy, consistency, and training speed  

- **Understanding XGBoostâ€™s Strength:**
  - Learned why tree-based models outperform deep learning on tabular data  
  - Understood feature interaction modeling, natural handling of heterogeneous data, and boosting advantages  

- **NoSQL & DynamoDB Knowledge:**
  - Studied NoSQL principles and scalability benefits  
  - Learned DynamoDB core concepts: partitioning, RCU/WCU, on-demand mode, and global tables  
  - Understood when NoSQL is preferred over relational databases  

- **Baseline XGBoost Model Completed:**
  - Built and tested a baseline model  
  - Tuned hyperparameters and recorded evaluation metrics  
  - Established a foundation for future training and model optimization  

- **Technical Competencies:**
  - Model comparison and selection  
  - Tree-based model theory  
  - NoSQL database design  
  - Hands-on DynamoDB concepts  
  - Baseline ML model development workflow  