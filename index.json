[{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"InsightHR Platform Overview InsightHR is a comprehensive serverless HR automation platform that demonstrates modern cloud-native application development on AWS. This workshop will guide you through building a production-ready application from scratch.\nProject Scope The InsightHR platform provides:\nEmployee Management System: Complete CRUD operations for employee records with advanced filtering by department, position, and status Performance Tracking: Quarterly performance scores with automatic calculation based on KPIs, completed tasks, and 360-degree feedback Attendance Management: Real-time check-in/check-out system with historical tracking and status monitoring AI-Powered Chatbot: Natural language query interface using AWS Bedrock (Claude 3 Haiku) for intelligent data insights Analytics Dashboard: Interactive visualizations with charts, tables, and export capabilities Role-Based Access Control: Three-tier access system (Admin, Manager, Employee) with appropriate data filtering Secure Authentication: Email/password and Google OAuth integration via AWS Cognito Architecture Overview â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ User Browser â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ CloudFront CDN â”‚ â”‚ Custom Domain: insight-hr.io.vn â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ S3 Static Website â”‚ â”‚ React SPA (Vite + TypeScript) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ API Gateway (REST) â”‚ â”‚ Cognito Authorizer â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Auth â”‚ â”‚ Employees â”‚ â”‚ Chatbot â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ DynamoDB â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Users â”‚ â”‚Employees â”‚ â”‚ Scores â”‚ â”‚Attendanceâ”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ AWS Bedrock â”‚ â”‚ (Claude 3 Haiku Model) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Technology Stack Frontend:\nReact 18 with TypeScript Vite 7.2 (build tool) Tailwind CSS 3.4 (Frutiger Aero theme) Zustand 5.0 (state management) React Hook Form + Zod (form validation) Recharts 3.4 (data visualization) Backend:\nPython 3.11 (Lambda runtime) AWS Lambda (serverless compute) AWS API Gateway (REST API) AWS DynamoDB (NoSQL database) AWS Cognito (authentication) AWS Bedrock (AI/ML) Infrastructure:\nAWS S3 (static hosting) AWS CloudFront (CDN) AWS Route53 (DNS) AWS CloudWatch (monitoring) AWS IAM (security) Key Features 1. Fully Serverless Architecture No EC2 instances to manage Automatic scaling based on demand Pay-per-use pricing model High availability built-in 2. Modern Development Stack TypeScript for type safety React for responsive UI Python for backend logic Infrastructure as Code principles 3. Production-Ready Custom domain with SSL CloudWatch monitoring Synthetic canaries for testing Role-based access control 4. Cost-Effective DynamoDB on-demand pricing Lambda free tier eligible Minimal monthly costs (~$2-5) No idle resource charges Workshop Structure This workshop is divided into 11 modules:\nWorkshop Overview (Current) - Understanding the project scope Prerequisites - Setting up your environment Project Architecture - Deep dive into system design Setup AWS Environment - Configuring AWS account and credentials Database Setup - Creating and populating DynamoDB tables Authentication Service - Implementing Cognito and auth Lambda functions Backend Services - Building employee, performance, and chatbot APIs Frontend Development - Creating the React application Deployment - Deploying to S3 and CloudFront Testing \u0026amp; Monitoring - Setting up CloudWatch and canaries Cleanup - Removing resources to avoid charges Learning Objectives By the end of this workshop, you will be able to:\nDesign and implement serverless architectures on AWS Build RESTful APIs using Lambda and API Gateway Model data effectively in DynamoDB Implement authentication with AWS Cognito Integrate AI capabilities using AWS Bedrock Deploy static websites with S3 and CloudFront Monitor applications with CloudWatch Apply security best practices with IAM Optimize costs for serverless applications Prerequisites Check Before proceeding, ensure you have:\nâœ… AWS Account with admin access âœ… AWS CLI installed and configured âœ… Node.js 18+ and npm installed âœ… Python 3.11+ installed âœ… Basic understanding of React and TypeScript âœ… Familiarity with REST APIs âœ… Text editor or IDE (VS Code recommended) Estimated Costs Running this workshop will incur minimal costs:\nService Estimated Cost DynamoDB $0.50/month Lambda Free tier S3 + CloudFront $1-2/month API Gateway $0.10/month Bedrock $0.0004/query Total $2-5/month Remember to complete the cleanup module at the end to avoid ongoing charges.\nNext Steps Ready to begin? Let\u0026rsquo;s move to the Prerequisites section to set up your development environment.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Huynh Thien Quang\nPhone Number: 0975400508\nEmail: quangkootenhatvutru@gmail.com\nUniversity: FPT University\nMajor: Artificial Inteligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will find my complete worklog documenting my AWS First Cloud Journey internship from September 8 to December 19, 2025. This represents a 12-week comprehensive learning experience covering AWS fundamentals, advanced services, and real-world application development through the InsightHR-1 project.\nWeek 1: Onboarding and AWS Foundation\nWeek 2: Development Environment Setup and AWS Cloud Day 2025\nWeek 3: VPC and EC2 Fundamentals with Static Website Deployment\nWeek 4: AWS Security Services and Identity Management\nWeek 5: Data Protection, Backup Services, and Messaging\nWeek 6: RDS Database Management, Auto Scaling, and CloudWatch Monitoring\nWeek 7: Serverless Architecture - Lambda, S3, and DynamoDB\nWeek 8: Midterm Review and Architectural Design\nWeek 9: Advanced Monitoring with CloudWatch and Grafana\nWeek 10: AWS Lambda with Python and Serverless Dashboard Development\nWeek 11: Lambda Functions Development and Docker Containerization\nWeek 12: Amazon Bedrock Chatbot and Project Finalization\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/3-blogstranslated/3.3-blog3/","title":"Blog 3: MasterClass enhances video quality and reduces costs with AWS Media Services","tags":[],"description":"","content":" MasterClass enhances video quality and reduces costs with AWS Media Services by Dan Gehred and Matt Carter on 25 MAR 2025 in Amazon CloudFront, AWS Elemental MediaConvert, Direct-to-Consumer \u0026amp; Streaming, Industries, Media \u0026amp; Entertainment, Media Services, Networking \u0026amp; Content Delivery\nFor many, cooking alongside Gordon Ramsay or writing with Shonda Rhimes may seem like pure fantasy, but streaming platform MasterClass makes these experiences a reality. The streaming platform offers more than 200 classes, delivered by the worldâ€™s best instructors. With video at the core of the MasterClass experience, the company looked to Amazon Web Services (AWS) Media Services to help support its continued growth.\nThe company now uses AWS Elemental MediaConvert and Amazon CloudFront to support video transcoding and delivery, with AWS Partner Nomad Media as an orchestration layer. After migrating to AWS, including the transfer of its archive content, MasterClass saw significant video infrastructure improvements, including enhanced video quality; it also reduced video costs by more than half.\nâ€œAWS provides a flexible solution that we can build on, so that we can create our own technology and IP around our media space,â€ said Paul Phipps, Senior Staff Software Engineer at MasterClass. â€œWith MediaConvert and CloudFront specifically, weâ€™ve dramatically reduced costs while improving streaming quality. Weâ€™ve also seen a reduction in buffering times and faster streaming across a variety of devices.â€\nImproved transcoding efficiency with AWS Elemental MediaConvert When MasterClass launched in 2015, the company used a turnkey video solution to develop the platform. While that setup served them well for many years, the companyâ€™s needs grew as its content library expanded. Making more video classes available to subscribers required a more flexible and scalable solution, which it found in the file-based transcoding service AWS Elemental MediaConvert.\nToday, MasterClass relies heavily on the Quality-Defined Variable Bitrate (QVBR) feature in MediaConvert. QVBR automatically allocates bits to maintain consistent video quality, confirming viewers enjoy an optimized experience, whether watching on a smart TV, computer, or mobile device. Previously, MasterClass had to first process files with FFmpeg, resulting in two compression steps. With its new setup, MasterClass sends native files directly into MediaConvert, leading to noticeably improved video quality, reduced artifacts, and faster content delivery for users.\nSeamless migration and enhanced content delivery Along with providing the orchestration layer, Nomad Media proved essential to a smooth transition, providing tools that enabled MasterClass to parallelize workflows with minimal disruption.\nâ€œWe re-transcoded all of our content onto the cloud, so we could improve the quality of our legacy content with QVBR,â€ explained Phipps. â€œOne of the great things about AWS is that every service is supported by a large infrastructure and serverless applications. Backed by that support, we were able to transcode most of our library in just a couple of days.â€\nNomadâ€™s direct integration with the content delivery network (CDN) Amazon CloudFront also influenced MasterClass while mapping out its migration. After evaluating several third-party CDNs, MasterClass determined that Amazon CloudFront provided the best fit.\nâ€œWe found that Amazon CloudFront performed better or as well as the other CDNs we assessed, and it was much easier to implement in our workflow since we were already using AWS,â€ noted Phipps. â€œAlso, there isnâ€™t an associated storage consumption cost, which is a huge benefit.â€\nMasterClass was interested in a system that allowed for a high rate of cache reuse. Using CloudFront with Lambda@Edge, the company optimized its token authentication process. With this setup, its team can refresh tokens without changing the content path, enabling secure access and maintaining cache stability.\nLooking ahead with AWS With full autonomy over its media stack, MasterClass can now customize its video workflows and explore new feature implementations like content analysis powered by artificial intelligence (AI) or live streaming with real-time interactivity. As MasterClass continues to innovate and expand, AWS provides them with robust and flexible infrastructure that supports both their present needs and future ambitions.\nâ€œHaving control over our own tech stack is essential for scaling our offerings and continuing to innovate,â€ concluded Phipps. â€œAWS gives us the flexibility we need to build a future-proof solution that supports our continued growth.â€\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week Duration: September 8 - September 14, 2025\nWeek 1 Objectives: Connect and get acquainted with AWS Study Group members. Create and secure an AWS account. Form a team for the final project. Gain a basic understanding of AWS core services. Learn how to read AWS documentation and explore resources effectively. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Create AWS Free Tier account - Explore basic AWS services and their use cases 09/08/2025 09/08/2025 https://000001.awsstudygroup.com/ Tuesday - Meet with the team - Get to know members and discuss initial project ideas 09/09/2025 09/09/2025 â€” Wednesday - Continue exploring AWS account setup - Understand IAM basics, service groups, and billing structure 09/10/2025 09/10/2025 https://000001.awsstudygroup.com/ Thursday - Learn about AWS Budget types - Understand how cost, usage, and savings-related budgets work 09/11/2025 09/11/2025 https://000007.awsstudygroup.com/ Friday - Deep dive into budget configuration - Learn when to use each budget type, its benefits, and limitations - Explore alerts and integrations with SNS 09/12/2025 09/12/2025 https://000007.awsstudygroup.com/ Week 1 Achievements: Gained a clear understanding of AWSâ€™s service categories, including:\nCompute Storage Networking Databases Machine Learning Monitoring \u0026amp; Billing â€¦ Successfully created a secured AWS account and learned to:\nconfigure IAM users and groups assign policies and permissions enable MFA for additional security Became familiar with the AWS Management Console and how to navigate major services.\nLearned the fundamentals of AWS Budgets, including:\nCost Budget â€“ used to limit monthly spending and receive alerts when exceeding targets. Usage Budget â€“ monitors resource usage such as S3 storage or EC2 hours. Savings Plans \u0026amp; RI Budgets â€“ tracks spending and commitment utilization for long-term compute plans. RI Coverage \u0026amp; Utilization Budgets â€“ evaluates how effectively Reserved Instances are used in an account. Understood when each budget type is useful: Cost Budgets â†’ ideal for beginners to avoid unexpected charges Usage Budgets â†’ useful for auto-scaling or high-volume workloads Reservation Budgets â†’ suited for predictable, long-term workloads Learned how AWS Budgets integrates with: email alerts SNS notifications Cost Explorer visualizations Formed a team for the final project and established initial communication channels.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/3-blogstranslated/3.1-blog1/","title":"Blog : Meeting the AWS Team","tags":[],"description":"","content":"Meet the AWS News Blog team! By: Channy Yun (ìœ¤ì„ì°¬)\nDate: 01 APR 2025\nCategories: Announcements, Featured, News\nNow that Jeff Barr has retired from the AWS News Blog as of December last year, the AWS News Blog team will keep sharing the most important and impactful AWS product launches the moment they become available. I want to quote Jeffâ€™s last comment on the future of the News Blog again:\nGoing forward, the team will continue to grow and the goal remains the same: to provide our customers with carefully chosen, high-quality information about the latest and most meaningful AWS launches. The blog is in great hands and this team will continue to keep you informed even as the AWS pace of innovation continues to accelerate.\nSince 2016, Jeff has been building the AWS News Blog as a team. Currently, weâ€™re a group of 11 bloggers working in North America, South America, Asia, Europe, and Africa. We co-work with AWS product teams, testing new features firsthand on behalf of customers, and delivering key details in the News Blog the way Jeff has always done.\nVoices from blog authors You may be familiar with the names of News Blog authors, but you may not have had the chance to hear about them. Let us introduce ourselves!\nChanny Yun (ìœ¤ì„ì°¬) Iâ€™m honored to continue Jeffâ€™s legacy as the new lead blogger of the News Blog team; he is my role model. When I joined AWS in 2014, the first thing I did was to create the AWS Korea Blog and I started translating Jeffâ€™s blog posts into the Korean language. During the journey, I learned how to write accurate, honest, and powerful guides to help customers get started with new AWS products and features.\nDanilo Poccia Since my first News Blog post in 2018, I have learned so much by being part of this team. Working with product managers and service teams is always an amazing experience. I am interested in serverless, event-driven architectures, and AI/ML. Itâ€™s incredible how technologies like generative AI are becoming part of software development implicitly (through AI-enabled development tools) and explicitly (by using models in code).\nSÃ©bastien Stormacq Iâ€™m fortunate to have been a part of this team since 2019. When I donâ€™t write posts, I produce episodes of the AWS Developers Podcast and le podcast AWS en franÃ§ais. I also work with the teams for Amazon EC2 Mac, AWS SDK for Swift, and the CodeBuild and CodeArtifact teams trying to make the AWS Cloud easier to use for Apple developers. My pet project is the Swift Runtime for AWS Lambda.\nVeliswa Boya The Amazon Leadership Principles (LPs) guide all that we do here at AWS, including the work we do as authors of the News Blog. As a developer advocate, Iâ€™ve taken the guidance of the LPs and used it to guide members of the AWS community who are looking to create technical content, especially those new in their technical content creation journey.\nDonnie Prakoso Just like brewing coffee, being a blog author has been a mix of fun, challenge, and reward. Iâ€™ve been particularly fortunate to observe how customer obsession is built into AWS teams. Iâ€™ve seen how they work backwards, transforming your feedback into services or features. I genuinely hope that you enjoy reading our articles and look forward to the next chapter of the News Blog team.\nEsra Kayabali As an author, Iâ€™m committed to delivering timely information about the latest AWS innovations and launches to our global audience of builders, developers, and technology enthusiasts. I understand the importance of providing clear, accurate, and actionable content that helps you use AWS services effectively. Happy reading everyone!\nMatheus Guimaraes My specialties are .NET development and microservices, but Iâ€™ve always been a jack-of-all-trades and writing for this blog helps me to keep my knife sharp across all corners of modern technology, while also helping others do the same. Thousands of people read the AWS News Blog and use it as a go-to source to keep up with whatâ€™s new and to help them make decisions, so I know that what we are doing is meaningful work with huge impact.\nPrasad Rao Through my blogs, I strive to highlight not just the â€œwhatâ€ of new services, but also the â€œwhyâ€ and â€œhowâ€ they can transform businesses and user experiences. As a solutions architect specializing in Microsoft Workloads on AWS, I help customers migrate and modernize their workloads and build scalable architecture on AWS. I also mentor diverse people to excel in their cloud careers.\nElizabeth Fuentes Every time I start writing a new blog, I feel honored to be part of this team, to be able to experiment with something new before itâ€™s released, and to be able to share my experience with the reader. This team is made up of specialists of all levels and from multiple countries and together, we are a multicultural and multi-specialty team. Thank you, reader, for being here.\nBetty Zheng (éƒ‘äºˆå½¬) Joining the News Blog team has transformed how I communicate about technology. With an ever-curious mindset, I approach each new announcement aiming to make innovative services accessible and engaging. By bringing my unique and diverse perspective to technical content, I strive to help developers truly enjoy exploring our latest technologies.\nMicah Walter As a senior solutions architect, I support enterprise customers in the New York City region and beyond. I advise executives, engineers, and architects at every step along their journey to the cloud, with a deep focus on sustainability and practical design.\nI also want to give credit to our behind-the-scenes editor-in-chief, Jane Watson, and program manager, Jane Scolieri, who play an essential role in helping us get product launch news to you as soon as it happens, including the 60 launches we announced in one week at re:Invent 2024!\nShare your feedback At AWS, we are customer obsessed. Weâ€™re always focused on improving and providing a better customer experience, and we need your feedback to do so. Take our survey to share insights about your experience with the AWS News Blog and suggestions for how we can serve you even better.\nThis survey is hosted by an external company. AWS handles your information as described in the AWS Privacy Notice. AWS will own the data gathered via this survey and will not share the information collected with survey respondents.\nâ€” Channy\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/3-blogstranslated/3.2-blog2/","title":"Blog Váº­n hÃ nh AWS Cloud â€“ Quáº£n lÃ½ AWS Config Rules with Remediation","tags":[],"description":"","content":" Manage Custom AWS Config Rules with Remediation Using AWS Config Conformance Pack TÃ¡c giáº£: Eswar Sesha Sai Kamineni, Ravindra Kori, Samrat Lamichhane, and Prathik Chintha NgÃ y: 05 MAY 2025 ChuyÃªn má»¥c: AWS Command Line Interface, AWS Config, Centralized operations management, Configuration, compliance, and auditing, Management Tools\nIntroduction Organizations face unique compliance requirements across their AWS resources and accounts. While AWS Config provides managed rules, many organizations need custom rules and automated remediation capabilities that can scale across their AWS Organization. This blog post demonstrates how to use AWS Config custom conformance pack to deploy and manage custom rules with remediation actions across organization-wide while maintaining centralized control.\nSolution Overview This solution implements a centralized compliance framework using:\nAWS Config custom rules with automated remediation Conformance packs for organization-wide deployment AWS Lambda functions for evaluation AWS Systems Manager for remediation The AWS Config custom rule configured in member accounts, invokes the evaluation Lambda function residing in the AWS delegated administrator account. The evaluation Lambda function residing in delegated admin account assesses target resources (e.g. AWS EC2 Security Group) against predefined compliance criteria and updates the evaluation result as either â€œCompliantâ€ or â€œNon-Compliantâ€ in AWS Config member account. Upon detection of non-compliant resources in the member account, the AWS Config rule will automatically trigger a remediation workflow through AWS Systems Manager Automation document. The AWS Systems Manager Automation document will trigger a Lambda function which is maintained in the delegated admin account, executes the defined actions to bring resources into compliant.\nKey Components There are two key components, delegated admin and member accounts\nDelegated admin account stores and manages: Lambda functions for rule evaluation and remediation AWS Config custom rule logic Systems Manager Automation runbooks Member Accounts: AWS Config enabled Cross-account access for centralized management In this blog post, we will demonstrate the solution by implementing a compliance control that checks for AWS EC2 Security group inbound rules. The rule identifies and remediates security groups allowing access from CIDR blocks larger than /16 (e.g., 10.0.0.0/10 or 10.0.0.0/1).\nImplementation Guide Prerequisites Verify access in AWS delegated admin and member accounts. Enable AWS Config in delegated admin and member accounts either through AWS CLI or AWS Management console Make a note of AWS Delegated admin account ID. Step 1: Deploy resources in the delegated admin account Download the CloudFormation template from this link. Sign in to the AWS Management Console and navigate to the CloudFormation service. Choose Create stack \u0026gt; With new resources (standard). Under Specify template, choose Upload a template file. Select the downloaded template and choose Next. The CloudFormation stack creates several interconnected components that work together to enable our compliance solution:\nFirst, it deploys two essential Lambda functions:\nConformancePackSecurityGroup function serves as our compliance evaluator, examining security group rules against our defined requirements. AutomationSecurityGroupConformance function handles remediation, automatically fixing any non-compliant resource configurations. To support these functions, the stack creates three IAM roles with specific permissions:\nConformancePackSecurityGroupLambdaRole enables security group evaluation AutomationSecurityGroupConformanceLambdaRole permits security group modifications SSMDocumentRole facilitates Systems Manager automation execution Finally, it creates the SecurityGroupAutomation SSMDocument in Systems Manager, which defines our automated remediation workflows.\nStep 2: Deploy cross-account IAM roles using CloudFormation StackSets Download the CloudFormation StackSet template from this link. Follow the steps in the documentation to create StackSets. Enter AWS Delegated Admin Account ID in parameters. Deploy StackSet across AWS Organizational Units. Step 3: Configure Lambda Permissions aws lambda add-permission \\ --function-name \u0026#34;ConformancePackSecurityGroupFunction\u0026#34; \\ --statement-id \u0026#34;AllowConfigCrossAccount\u0026#34; \\ --action \u0026#34;lambda:InvokeFunction\u0026#34; \\ --principal \u0026#34;config.amazonaws.com\u0026#34; \\ --source-account ${MEMBER_ACCOUNT_ID} Step 4: Share AWS Systems Manager Automation Runbook aws ssm modify-document-permission \\ --name \u0026#34;SecurityGroupAutomationSSMDocument\u0026#34; \\ --permission-type Share \\ --account-ids-to-add \u0026#34;111111111111\u0026#34; \u0026#34;222222222222\u0026#34; \u0026#34;333333333333\u0026#34; Step 5: Deploy the Conformance Pack aws configservice put-organization-conformance-pack \\ --organization-conformance-pack-name \u0026#34;SecurityGroupConformancePack\u0026#34; \\ --template-body file://SecurityGroupConformancePack.yaml \\ --delivery-s3-bucket YOUR-S3-BUCKET-NAME \\ --conformance-pack-input-parameters \\ ParameterName=ManagementAccountId,ParameterValue=YOUR-Management-ACCOUNT-ID Testing the Solution To verify that the AWS Config Conformance Pack will detect and remediate the security group rule, letâ€™s test it by creating a non-compliant security group and evaluating the automated remediation process.\nFirst, create a test security group in a member account:\nNavigate to the EC2 console and create a new security group Add an inbound rule allowing SSH (port 22) from 10.0.0.0/8 Tag the security group appropriately for identification Within few minutes of creation, AWS Config will evaluate this security group.\nIn the member account:\nCheck the AWS Config console Evaluate the security group status change from â€œCompliantâ€ to â€œNon-compliantâ€ In the delegated admin account:\nReview CloudWatch Logs for both Lambda functions Confirm the evaluation detected the non-compliant CIDR Verify the remediation action completed successfully The solution should automatically detect and remove the CIDR range (10.0.0.0/8), maintaining only compliant rules (those with /16 or smaller CIDR blocks).\nCleanup Delete the conformance pack Delete the CloudFormation Stack Empty and delete the S3 bucket used for the conformance pack delivery Conclusion In this blog post, we demonstrated a solution for implementing organization-wide compliance using AWS Config conformance packs. By combining AWS Config, Lambda, Systems Manager, and AWS Organizations, we created a scalable framework that automatically detects and remediates security group misconfigurations across multiple AWS accounts. The solution showcases how to centrally manage custom compliance rules while maintaining automated remediation workflows.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/4-eventparticipated/4.1-event1/","title":"Event: Vietnam Cloud Day 2025","tags":[],"description":"","content":"Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders - GenAI and Data Track Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Thursday, September 18, 2025\nLearning Report: \u0026ldquo;Vietnam Cloud Day 2025: GenAI and Data Track\u0026rdquo; Event Goals Present an overview of Agentic AI and AWSâ€™s perspective on AI trends Learn how to create a Unified Data Foundation to enable AI initiatives and analytics on AWS Examine the GenAI deployment roadmap, AI Agent Architecture, and challenges in production adoption Explore the AI-Driven Development Lifecycle (AI-DLC) model Understand key practices for Security, Risk Management, and Responsible AI in Generative AI Introduce new AWS services designed to enhance AI Agents and business efficiency Speaker Roster Jun Kai Loke - AI/ML Solution Architecture Expert, AWS Kien Nguyen - Solution Architect, AWS Tamelly Lim - Storage Solution Architecture Expert, AWS Binh Tran - Senior Solution Architect, AWS Taiki Dang - Solution Architect, AWS Christal Poon - Solution Architecture Expert, AWS Key Takeaways from Presentations Agentic AI Overview â€“ Jun Kai Loke Agentic AI focuses on building autonomous systems that minimize human involvement and automate complex workflows Successful real-world examples: Techcombank, ELSA Speak Amazon Bedrock as the primary AI platform, offering: Secure and scalable deployment Tool and memory integration Full end-to-end monitoring Creating a Unified Data Foundation â€“ Kien Nguyen Current Issues: Many organizations are unprepared for GenAI deployment; only 52% of CDOs rate their platforms as ready (HBR) Main obstacles: data silos, personnel silos, disconnected business units Comprehensive Data Strategy: Combines Producers, Foundations, and Consumers Key AWS Data Services: Amazon Bedrock (GenAI platform) Databases (RDS and specialized vector-search support) Analytics \u0026amp; ML (SageMaker, Unified Studio) Data \u0026amp; AI Governance Lake House Architecture (S3, Redshift, Iceberg API) Amazon DataZone (Data management and sharing) GenAI Roadmap \u0026amp; AI Agent Architecture â€“ Jun Kai Loke \u0026amp; Tamelly Lim Blueprint for AI Agents: Model \u0026amp; application capabilities, plus tool frameworks Amazon Bedrock AgentCore helps tackle production challenges AgentCore Components: Runtime, Gateway, Memory, Agent Browser, Code Interpreter; enhances security and scalability AI-Driven Development Lifecycle (AI-DLC) â€“ Binh Tran AI-DLC automates software development, comprising three stages: Inception: Define context, user stories, and plan work units Construction: Code, test, enhance architecture, deploy IaC, and validate Operation: Production deployment with IaC and incident management Security in Generative AI â€“ Taiki Dang Core Security Elements: Compliance, governance, legal/privacy, controls, risk management, resilience Risk Assessment by Layer: End-user: hallucination, IP, legal issues Fine-tuning: data retention Model provider: training data, model construction Mitigation Techniques: Prompt engineering, fine-tuning, RAG, parameter adjustments, Bedrock Guardrails, secure prompts Standards to follow: AWS Well-Architected, MITRE ATLAS, OWASP Top 10 LLM Apps, NIST AI 600-1, ISO 42001, EU AI Act AI Agents for Business Productivity â€“ Christal Poon AI Agent categories: Specialized, Fully-managed, DIY Productivity tools: Amazon QuickSight (analytics), Amazon Q (dashboards, reports, summaries, AI scenarios) Achievements Mindset \u0026amp; Strategy Agentic AI can automate routine processes, allowing focus on strategic work Robust data platforms are crucial to handle growing data volumes securely (e.g., S3) AI-DLC automates development from planning to deployment Architecture \u0026amp; Technology Understanding AI Agent architecture and AgentCore resolves production security and scalability issues Security integration is required at all levels of the AI stack, adhering to international regulations Technology Application AWS is expanding the AI Agents \u0026amp; Enterprise AI ecosystem with services like Amazon Q and QuickSuite (upcoming) Application to Work Process Optimization: Integrate AI Agents into repetitive tasks Quality Control: Leverage Bedrock, Amazon Q, Guardrails to ensure quality and prevent hallucinations Infrastructure Development: Build a unified data platform with clear governance for GenAI projects Implement AI-DLC: Apply AI-DLC methodology in internal development Business Insights: Use QuickSight and Amazon Q for dashboards and reports Event Experience The workshop offered practical guidance for transitioning from traditional automation to Agentic AI. Presentations on AgentCore, Bedrock, AI-DLC, and Amazon Q demonstrated the full spectrum of AI adoption, from data platforms to secure operations.\nAdd your event photos here The event provided deep technical insights and encouraged strategic, responsible integration of AI into business processes.\nAdditional Speakers Jignesh Shah â€“ Director, Open Source Databases Erica Liu â€“ Sr. GTM Specialist, AppMod Fabrianne Effendi â€“ Assoc. Specialist SA, Serverless AWS Key Points Legacy Architecture Challenges Slow release cycles â†’ missed opportunities Inefficient operations â†’ higher costs and lower productivity Security gaps â†’ breaches and reputation risks Modern Architecture â€“ Microservices Modular design: independent services communicating via events Core pillars: Queue Management, Caching, Message Handling Domain-Driven Design (DDD) 4-step method: Identify domain events â†’ timeline â†’ actors â†’ bounded contexts Bookstore case study demonstrates DDD in action Context mapping: 7 patterns for bounded context integration Event-Driven Architecture Integration patterns: Pub/Sub, point-to-point, streaming Benefits: loose coupling, scalability, resilience Sync vs async trade-offs Compute Evolution Shared Responsibility: EC2 â†’ ECS â†’ Fargate â†’ Lambda Serverless: auto-scaling, no server maintenance Choosing between functions and containers Amazon Q Developer SDLC automation: planning to maintenance Code transformation: Java, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Lessons Design Thinking Business-first mindset: start with domain, not tech Shared vocabulary: unify communication between teams Bounded contexts: manage large-system complexity Technical Insights Event storming to model business processes Event-driven communication over synchronous calls Appropriate integration pattern selection Compute choice: VM, containers, serverless Modernization Approach Phased modernization: follow a clear roadmap 7Rs framework: choose the best path per application ROI assessment: measure cost reduction and agility gains Applying Insights at Work Conduct DDD sessions with business teams Define microservice boundaries using bounded contexts Replace sync calls with async messaging patterns Pilot AWS Lambda for serverless projects Integrate Amazon Q Developer into workflows Event Experience The â€œGenAI-powered App-DB Modernizationâ€ workshop was highly valuable, showing modern techniques for application and database modernization.\nLearning from Experts AWS and industry professionals shared best practices in modern architecture Case studies deepened understanding of DDD and Event-Driven Architecture Hands-on Exposure Event storming sessions helped translate business processes into domain events Learned how to split microservices and define bounded contexts Explored sync vs async communication and patterns (pub/sub, point-to-point, streaming) Leveraging Tools Tested Amazon Q Developer for SDLC support Learned to automate code modernization and pilot serverless Networking Exchanged ideas with experts and peers, improving shared vocabulary Real-world cases emphasized business-first approach Lessons Learned DDD and event-driven patterns improve scalability and reduce coupling Modernization requires phased execution and ROI tracking AI tools like Amazon Q Developer can boost productivity Event Photos Image 1 Image 2 Image 3 Image 4 Overall, the workshop provided both technical knowledge and a new perspective on application design, system modernization, and team collaboration.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.2-prerequisite/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for InsightHR Workshop Before starting this workshop, ensure you have the following tools and accounts set up.\n1. AWS Account You\u0026rsquo;ll need an AWS account with appropriate permissions to create and manage resources.\nRequired AWS Services Access:\nIAM (Identity and Access Management) DynamoDB Lambda API Gateway S3 CloudFront Cognito Bedrock CloudWatch Route53 (optional, for custom domain) Estimated Costs: $2-5/month during development\nIf you\u0026rsquo;re using AWS Free Tier, many services in this workshop are covered. However, some services like Bedrock may incur small charges.\n2. AWS CLI Install and configure the AWS Command Line Interface.\nInstallation:\nWindows:\nmsiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\ncurl \u0026#34;https://awscli.amazonaws.com/AWSCLI2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configuration:\naws configure Enter your:\nAWS Access Key ID AWS Secret Access Key Default region (e.g., ap-southeast-1) Default output format (e.g., json) Verify Installation:\naws --version # Expected output: aws-cli/2.x.x Python/3.x.x ... 3. Node.js and npm Required for frontend development.\nMinimum Version: Node.js 18+\nInstallation:\nDownload from nodejs.org or use a version manager:\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js nvm install 18 nvm use 18 Verify Installation:\nnode --version # Expected: v18.x.x or higher npm --version # Expected: 9.x.x or higher 4. Python Required for Lambda function development.\nMinimum Version: Python 3.11+\nInstallation:\nDownload from python.org or use your system\u0026rsquo;s package manager.\nVerify Installation:\npython --version # or python3 --version # Expected: Python 3.11.x or higher Install pip (if not included):\npython -m ensurepip --upgrade 5. Text Editor / IDE Choose your preferred development environment:\nRecommended: Visual Studio Code\nDownload from code.visualstudio.com Install recommended extensions: AWS Toolkit Python ESLint Prettier Tailwind CSS IntelliSense Alternatives:\nPyCharm Sublime Text Atom WebStorm 6. Git (Optional but Recommended) For version control and accessing code repositories.\nInstallation:\nDownload from git-scm.com\nVerify Installation:\ngit --version # Expected: git version 2.x.x 7. Required Knowledge JavaScript/TypeScript:\nBasic syntax and ES6+ features Async/await and Promises React fundamentals (components, hooks, state) Python:\nBasic syntax and data structures Functions and error handling Working with JSON AWS Concepts:\nBasic understanding of cloud computing Familiarity with AWS Console Understanding of serverless architecture (helpful but not required) Web Development:\nHTML/CSS basics REST API concepts HTTP methods (GET, POST, PUT, DELETE) 8. AWS Account Setup Create IAM User For security best practices, create an IAM user instead of using root credentials:\nSign in to AWS Console\nNavigate to IAM service\nClick \u0026ldquo;Users\u0026rdquo; â†’ \u0026ldquo;Add users\u0026rdquo;\nEnter username (e.g., insighthr-admin)\nSelect \u0026ldquo;Programmatic access\u0026rdquo; and \u0026ldquo;AWS Management Console access\u0026rdquo;\nAttach policies:\nAdministratorAccess (for workshop purposes) Or create a custom policy with required permissions Download credentials (Access Key ID and Secret Access Key)\nConfigure AWS CLI with these credentials\nEnable Required Services Ensure the following services are available in your region:\nâœ… AWS Lambda âœ… Amazon DynamoDB âœ… Amazon API Gateway âœ… Amazon S3 âœ… Amazon CloudFront âœ… Amazon Cognito âœ… Amazon Bedrock (check regional availability) âœ… Amazon CloudWatch Recommended Region: ap-southeast-1 (Singapore) - All services are available and latency is good for Southeast Asia.\n9. Bedrock Model Access AWS Bedrock requires explicit model access request.\nSteps to Enable:\nGo to AWS Console â†’ Amazon Bedrock Navigate to \u0026ldquo;Model access\u0026rdquo; in the left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box and click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant for Haiku) Without Bedrock access, the AI chatbot feature won\u0026rsquo;t work. However, you can still complete the rest of the workshop.\n10. Optional: Domain Name If you want to use a custom domain (like insight-hr.io.vn):\nPurchase a domain from a registrar (e.g., Route53, GoDaddy, Namecheap) Have access to DNS management Budget for SSL certificate (free with AWS Certificate Manager) Pre-Workshop Checklist Before proceeding to the next section, verify you have:\nAWS account with admin access AWS CLI installed and configured Node.js 18+ and npm installed Python 3.11+ installed Text editor/IDE set up Git installed (optional) Basic knowledge of JavaScript/TypeScript and Python Understanding of REST APIs AWS Bedrock model access requested Familiarity with AWS Console Troubleshooting AWS CLI Configuration Issues:\n# Check current configuration aws configure list # Test AWS access aws sts get-caller-identity Node.js Version Issues:\n# Check installed versions nvm list # Switch to correct version nvm use 18 Python Version Issues:\n# Check Python path which python3 # Create virtual environment python3 -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Next Steps Once you\u0026rsquo;ve completed all prerequisites, proceed to Project Architecture to understand the system design.\nAdditional Resources AWS CLI Documentation Node.js Documentation Python Documentation AWS Free Tier AWS Bedrock Documentation "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/2-proposal/","title":"Proposal","tags":[],"description":"","content":"ğŸ“¥ Download Proposal\nAWS First Cloud AI Journey â€“ Project Plan [SKYLINE2] â€“ [University] â€“ [INSIGHTHR]\n[09-12-2025]\nTable of Contents BACKGROUND and motivation\n1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations Activities AND Deliverables\n3.1 Activities and deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN BY SERVICES\nTEAM\nRESOURCES \u0026amp; COST ESTIMATES\nACCEPTANCE\nBACKGROUND AND MOTIVATION 1.1 Executive Summary Organization faces HR evaluation inefficiencies due to manual data handling, lack of transparency in evaluation processes and metrics tracking.\nInsightHR delivers HR automation through flexible evaluation management, automated scoring, and AI insights. AWS provides serverless scalability, cost efficiency, security for sensitive data, AI Chatbot via Bedrock, and rapid deployment.\nCustom KPI, automated performance scoring, multi-level dashboards, AI assistant for natural-language queries, automated notifications, role-based access (Admin/Manager/Employee), multi-tenant support.\nEnd-to-end delivery including Well-Architected design, serverless backend (Lambda, DynamoDB, API Gateway), frontend (S3 + CloudFront), authentication/security (Cognito, IAM), KPI/formula builder, AI chatbot (Bedrock + Lambda query data from info tables), notifications (SNS, SES), CI/CD, monitoring, and knowledge transfer.\n1.2 Project Success Criteria Success is defined by demonstrating a functional MVP that proves the platform\u0026rsquo;s capability to automate HR evaluations and deliver measurable business value.\n1. Functional Criteria:\nAuthentication with role-based access (Admin/HR, Manager, Employee) HR creates custom KPIs without technical support CSV upload triggers automated Lambda scoring Dashboard displays individual/team performance with charts AI chatbot answers natural language queries from DynamoDB data SES sends automated email notifications 2. Technical Criteria:\n99.9%+ uptime \u0026lt;300ms API latency (95th percentile) 95%+ scoring accuracy vs manual calculations 90%+ AI response relevance Zero critical security vulnerabilities 3. Performance \u0026amp; Cost:\n~$33.14/month AWS cost End-to-end workflow (upload â†’ score â†’ visualize) completes in \u0026lt;5 minutes 4. Business Impact:\nDemonstrates 60%+ HR time reduction potential Non-technical users operate KPI builder and chatbot independently 5. Delivery:\nWeek 8: MVP (authentication, KPI/formula management, scoring, basic dashboard) Week 12: Full features (chatbot, notifications, advanced dashboard) 1.3 Assumptions 1. Assumptions:\nThe current AWS cost estimate of approximately $33.14/month is accurate for the projected initial load and usage. The required data format and mapping logic for employee performance data can be clearly defined and provided by the HR team for the automated scoring engine. The Large Language Model provided by Amazon Bedrock to support HR. The automated scoring system is trained locally. The technical evaluation files of each team are assessed according to the companies\u0026rsquo; criteria and must follow the format provided by the customer. 2. Constraints:\nThe project delivery must adhere to the 12-week timeline utilizing the Agile Scrum framework. The solution must be built entirely on serverless AWS services to meet the objectives of scalability, cost efficiency, and reduced operational overhead. The final production AWS cost must remain around the ~$33.14/month target. 3. Risks:\nData Security/Compliance: Failure to fully understand or implement all of the customer\u0026rsquo;s specific regulatory control validation requirements could impact the project\u0026rsquo;s ability to meet security objectives. Feature Creep: Requests for features identified as \u0026ldquo;Out of Scope\u0026rdquo; could derail the 12-week MVP delivery timeline. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The InsightHR platform is built on a serverless architecture using AWS services, providing scalability, cost-effectiveness, and high availability. The architecture includes:\n1. Frontend \u0026amp; Content Delivery:\nAmazon S3: Hosts the static website and stores user-uploaded files (CSV, AI models). S3 Vector to store vectors (embeddings for text-data), S3 Standard for storing raw documents. Amazon CloudFront: Distributes static and dynamic content globally with low latency. 2. Backend \u0026amp; Compute:\nAWS Lambda: Executes all business logic, including authentication, custom scoring, and chatbot functions. Amazon API Gateway: Manages APIs as the communication gateway between frontend and backend. 3. Data Storage:\nAmazon DynamoDB: Stores structured data such as user/employee information, company KPIs, scoring formulas, and performance evaluation results. 4. AI \u0026amp; Machine Learning:\nAmazon Bedrock: Provides Large Language Models (LLMs) for the HR assistant chatbot. The ML Model system is trained locally for scoring function. 5. Security \u0026amp; Identity:\nAmazon Cognito: Manages user authentication, registration, and identity workflows. AWS IAM: Manages access control and permissions for AWS services. AWS KMS: Encrypts sensitive data in DynamoDB and S3. 6. Monitoring \u0026amp; Notifications:\nAmazon CloudWatch \u0026amp; CloudWatch Logs: Monitors Lambda functions, API Gateway, and database access. Amazon SNS: Sends notifications (e.g., reminders, result notifications) to employees. 7. Architecture Benefits:\nServerless: No server management and automatic scaling. Cost-Effective: Mostly pay-as-you-go services. High Availability: Built-in redundancy across AWS regions. Scalable: Can handle growth from small teams to large enterprises. Flexible: Easy to modify and extend functionality. 8. Proposed Architecture Diagram:\n9. Tools Proposed for This Project:\nAmazon CloudFront: For global content delivery and caching of static and dynamic web content. Amazon S3: To host static web assets and store documents, vector embeddings, and other files processed by the system. Amazon API Gateway: To provide a secure RESTful interface, acting as the communication layer between frontend clients and backend services. AWS Lambda: To run backend business logic including user dashboard, auto scoring, HR assistant, and data management workflows. Amazon DynamoDB: To store application data such as user information, HR records, scoring results, and vector metadata with low-latency performance. Amazon Cognito: To manage user authentication, authorization, registration, MFA, and secure access to APIs and frontend applications. AWS Identity and Access Management (IAM): To define fine-grained access policies and control permissions between services and users. AWS Key Management Service (KMS): To manage encryption keys used for securing sensitive data stored in S3, DynamoDB, and logs. Amazon ECR: To store containerized model assets and application dependencies in a secure and version-controlled repository. Amazon Bedrock / Large Language Model (LLM): To provide AI capabilities for chat, data extraction, summarisation, and intelligent HR workflows. Amazon Simple Email Service (SES): To send automated email notifications such as onboarding alerts and HR communications. Amazon Simple Notification Service (SNS): To publish notifications and trigger downstream processes; integrates with email, SMS, and microservices. Amazon CloudWatch \u0026amp; CloudWatch Logs: For monitoring performance, logging, tracing, and operational troubleshooting across Lambda, API Gateway, and AI components. 2.2 Technical Plan The partner will develop automated deployment scripts using AWS CloudFormation and Infrastructure as Code (IaC) practices.\nThis will allow for quick and repeatable deployments into AWS accounts. Some additional configurations such as WAF rules on CloudFront for enhanced security may require approval and will follow standard DevOps change management processes.\nApplication Feature Implementation:\n1. Authentication \u0026amp; Security Module\nUser Management: Cognito manages user lifecycle Registration, login, password reset workflows Access Control: IAM and RBAC enforce role-based permissions Admin/HR, Manager, and Employee access levels API Security: API Gateway implements JWT-protected endpoints Token validation before Lambda processing 2. Administration Module (HR Panel)\nKPI Management: HR creates, edits, and deletes custom metrics Examples: Tasks Completed, Code Quality, Customer Satisfaction Definitions stored in DynamoDB Auto scoring by employee\u0026rsquo;s technical score for each team with ML model. 3. Core User Functions\nData Upload \u0026amp; Mapping: Upload performance data files (CSV) to DynamoDB Scoring Engine: Lambda triggered on upload Retrieves active formula from DynamoDB Calculates employee scores Stores results in DynamoDB Flow: Upload â†’ Validate â†’ Map â†’ Calculate â†’ Store Dashboard: Visualize individual and department performance Line graphs, bar charts, trend analysis AI Chatbot: Bedrock (LLM) integration Natural language queries (e.g., \u0026ldquo;Summarize Team A Q4 performance\u0026rdquo;) Queries and summarizes DynamoDB data Notifications: SES sends automated alerts Performance milestones, review reminders, custom triggers 2.3 Project Plan The partner will adopt the Agile Scrum framework over 12 one-week sprints totaling a 12-week delivery timeline.\n1. Team Responsibilities\nProduct Owner: Prioritizes backlog (KPIs, formulas, analytics) Final authority on feature acceptance Development Team: Implements Cognito authentication Builds admin portal and formula builder Develops scoring engine and dashboard Integrates Bedrock chatbot and SNS with SES notifications via Email. QA Personnel: Conducts functional, performance, and security testing Facilitates UAT Ensures compliance and quality standards 2. Communication Cadences\nDaily Standups (30 min - 1 hr): Progress review and blocker identification Retrospectives (Weekly, 1 hr): Process improvement and delivery optimization Executive Updates (Weekly): Written reports on progress, risks, KPIs, roadmap Leadership decisions required 3. Knowledge Transfer\nSessions conducted by the development team covering AWS serverless fundamentals KPI and formula configuration Data workflows and column-mapping Dashboard navigation and analytics System monitoring (CloudWatch, Cognito, DynamoDB) 2.4 Security Considerations The partner will implement AWS security best practices based on the Well-Architected Framework, prioritizing protection of sensitive HR data while ensuring high operational availability. Security implementation covers five key categories:\n1. Access Control\nCognito manages user identities Enforces strong password policies and MFA support IAM implements RBAC Admin/HR access Admin Panel and KPI/Formula configurations Employees view only their own performance data API Gateway validates JWT tokens Cognito-issued tokens verified before Lambda processing 2. Infrastructure Security\nServerless architecture reduces attack surface No OS or server patching required Lambda functions communicate via private AWS networks Only necessary endpoints exposed through API Gateway 3. Data Protection\nKMS encrypts data at rest DynamoDB and S3 encrypted Data unusable without decryption keys TLS/SSL (HTTPS) encrypts data in transit All frontend-backend communication secured 4. Detection \u0026amp; Monitoring\nCloudWatch Logs captures execution details Lambda and API Gateway activity logged Real-time monitoring and anomaly detection enabled AWS Config tracks configuration changes Ensures resource compliance with security objectives 5. Incident Management\nCloudWatch Alarms trigger automated alerts via SES Failed login threshold breaches Lambda resource anomalies Security Hub provides consolidated security view Unified compliance findings across AWS environment Simplifies incident identification and response AWS CloudTrail and AWS Config will be configured for continuous monitoring of activities and compliance status of resources. The customer will share their regulatory control validation requirements as inputs for the partner to ensure all security objectives are met.\nACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables NOTE: Some Project Phases overlap each other.\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Phase 1: Foundation \u0026amp; Scoring Model Week 1-8 â€¢ Personal infrastructure architecture research â€¢ Data generation for local model training â€¢ Scoring model build â€¢ Finalized personal architecture diagram â€¢ Ready dataset for Local Model training â€¢ Scoring Model MVP (Minimum Viable Product) 80 Phase 2: Project Setup \u0026amp; Dashboard Week 9-10 â€¢ Project Setup with basic functions: IAM Role, CRUD function, Static web â€¢ Web UI Demo â€¢ Implement Dashboard â€¢ Fix Model â€¢ Basic IAM Roles configured â€¢ Operational CRUD functions â€¢ Static website deployed (S3/CloudFront) â€¢ Web UI Demo completed â€¢ Dashboard displaying data implemented 40 Phase 3: AI Agent \u0026amp; Absence Mgmt Week 11 â€¢ Building Bedrock Agent â€¢ Implement Absent managing â€¢ Bedrock Agent built â€¢ Operational Absence tracking workflow implemented 15 Phase 4: Integration, Testing \u0026amp; Handover Week 12 â€¢ Implement Chatbot into App â€¢ Testing and set up Monitoring â€¢ Chatbot integrated into the application â€¢ Functional, Performance, and Security Testing completed â€¢ Monitoring (CloudWatch) configured and operational â€¢ Project Completion Report \u0026amp; Post-implementation support plan delivered 15 3.2 Out Of Scope 1. AI Enhancements\nAI-Powered Insights:\nWhen sufficient data is available, develop AI models capable of: Chatbot accesses database directly and retrieves prompt window -\u0026gt; Price a lot of tokens and high lock, future can be optimized by other ways Identifying performance patterns across teams and departments Predicting HR risks (e.g., turnover likelihood, burnout indicators) Recommending personalized development plans Detecting anomalies in performance data Suggesting optimal team compositions Machine Learning Features:\nPredictive analytics for workforce planning Sentiment analysis from employee feedback Automated skill gap analysis Performance trend forecasting 2. Public API Development\nAPI Ecosystem:\nBuild a comprehensive API set allowing other internal business systems to automatically push performance data into InsightHR. Integration Targets:\nProject management tools (Jira, Asana, Monday.com) CRM systems (Salesforce, HubSpot) Time tracking software (Toggl, Harvest) Communication platforms (Slack, Microsoft Teams) Code repositories (GitHub, GitLab, Bitbucket) Benefits:\nTransform InsightHR into a central HR data processing hub Create a synchronized and comprehensive management ecosystem Eliminate manual data entry Real-time performance tracking 3. Advanced Features\nMobile Applications:\niOS and Android native apps Push notifications Offline capabilities Mobile-optimized dashboards DynamoDB back up Advanced Analytics:\nPredictive modeling Benchmarking across industries Custom report builder Data export and API for third-party tools Collaboration Features:\nPeer review systems 360-degree feedback Goal setting and tracking Performance improvement plans Compliance \u0026amp; Governance:\nAudit trails Compliance reporting Data retention policies Advanced access controls 3.3 Path To Production This document outlines the current production architecture and operational status for the InsightHR platform deployment. The platform is fully live in the ap-southeast-1 (Singapore) region.\n1. Platform Architecture and Access\nPublic URL: https://d7gdgmhloq3vn.cloudfront.net AWS Region: ap-southeast-1 (Singapore) Frontend: React application hosted on S3 (insighthr-web-app-sg) with CloudFront HTTPS distribution. Backend: 8 Lambda function groups accessed via API Gateway REST API. Database: DynamoDB tables configured with On-Demand capacity. 6 tables for each team Employee information table History score table Absent table Account managing tables Authentication: Cognito User Pool. AI/Chatbot: Amazon Bedrock (Claude 3 Haiku) integration for conversation history and knowledge base to enhance models. 2. Live Production Features\nThe following core features have been successfully deployed and are operational:\nAuthentication: Full support for email/password login, password reset workflows. User Management: Complete CRUD functionality, including bulk import and role-based access. Employee Management: Full support for 300+ employees and bulk operations. Performance Score Management: Management of 900+ quarterly scores and calendar-based viewing. Attendance Management: Processing of 9,300+ records, including check-in/check-out kiosk functionality and auto-absence marking. Performance Dashboard: Live charts, trend analysis, live clock, and CSV export capabilities. AI Chatbot: Bedrock integration with conversation history enabled. 3. Deployment and Verification Process\nThe standard, repeatable deployment workflow ensures rapid and verifiable updates to the production site:\nBuild: npm run build creates the optimized production asset bundle. Test: npm run preview validates the built bundle locally prior to deployment. Deploy: aws s3 sync dist/ s3://insighthr-web-app-sg --region ap-southeast-1 pushes assets to the S3 bucket. Invalidate: aws cloudfront create-invalidation --distribution-id E3MHW5VALWTOCI --paths \u0026quot;/*\u0026quot; clears the CloudFront CDN cache. Verify: Full feature testing is performed on the live public URL. 4. Remaining Production Enhancements\nThe platform is in the final phases of enhancement before full stabilization, with key items planned or in progress:\nPage Integration (In Progress)\nConsolidate all administrative page navigation. Verify all features are accessible from the main menu. Test role-based routing across all pages. Fix any integration bugs. Polish and Final Deployment (Planned)\nImplement comprehensive error handling and input validation. Refine responsive design for full mobile compatibility. Conduct dedicated Security testing (penetration testing, vulnerability scanning). Execute Load testing for scalability validation. Develop user documentation and training materials. Perform final production hardening procedures. Monitoring and Scalability Strategy\nActive Monitoring: CloudWatch Logs are enabled for all Lambda functions and API Gateway endpoints, along with CloudWatch Metrics for performance tracking. Planned Alarms: CloudWatch Alarms and SNS notifications are planned for critical error rates and latency. Scalability: Achieved via Serverless Architecture (DynamoDB On-Demand, Lambda, CloudFront CDN). Disaster Recovery: DynamoDB Point-in-Time Recovery and S3 Versioning are planned to be enabled for critical data/assets. Lambda code is stored in version control for rapid redeployment. EXPECTED AWS COST BREAKDOWN BY SERVICES AWS Service Monthly Estimated Cost (USD) Amazon Bedrock $21.61 AWS Lambda $3.75 Amazon Simple Email Service (SES) $2.25 Amazon DynamoDB $1.52 Amazon Simple Storage Service $0.46 Amazon CloudWatch $0.80 Amazon API Gateway $0.06 Amazon CloudFront $0.00 Amazon Cognito $0.00 Amazon EventBridge $0.00 Amazon IAM $1.60 Amazon KMS $1.03 TOTAL MONTHLY COST $33.14 TOTAL YEARLY COST $397.79 TEAM Name Task Role Email / Contact Info BÃ¹i Táº¥n PhÃ¡t Dashboard, Manage Employee, Support, Content check Leader btfat3103@gmail.com Nguyá»…n Ngá»c Long CRUD, Config Network / API Gateway, Test function, Slide Member nguyenngoclong216@gmail.com Äáº·ng Nguyá»…n Minh Duy Database, CloudWatch / CloudLogs, Paper, Slide Member dangnguyenminhduy11b08@gmail.com Äá»— ÄÄƒng Khoa Log In/ Registration / Forget Password, UI / UX - Static Web, Paper Member khoado7577@gmail.com Nguyá»…n Huá»³nh ThiÃªn Quang Auto Scoring, AI Assistant, Slide Member quangkootenhatvutru@gmail.com RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Full-Stack Developers [2] React frontend, Python Lambda backend, API integration $66 Cloud Engineers [3] AWS infrastructure setup, deployment automation, monitoring $66 Other (Please specify) Estimated platform consumption (Lambda, DynamoDB, Bedrock). Paper and present material $0.01 NOTE: Project Phase durations overlap each other.\nProject Phase Duration Man-Days Other (Please specify) Estimated Cost Phase 1: Foundation \u0026amp; Scoring Model 8 Weeks 80 - $42,246.40 (80 x $528.08) Phase 2: Project Setup \u0026amp; Dashboard 2 Weeks 40 - $21,123.20 (40 x $528.08) Phase 3: AI Agent \u0026amp; Absence Mgmt 1 Week 15 - $7,921.20 (15 x $528.08) Phase 4: Integration, Testing \u0026amp; Handover 1 Week 15 - $7,921.20 (15 x $528.08) Total Hours 12 Weeks 150 Man-Days $79,212.00 Cost Contribution distribution between Partner, Customer, AWS.\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 200 100 ACCEPTANCE 1. Project Acceptance Criteria\nThe InsightHR platform will be considered complete and accepted when the following criteria are met.\n2. Completed Deliverables\nAll major features implemented and deployed to production User and employee management with bulk operations Performance score management with calendar view Attendance system with auto-absence marking Interactive dashboard with live clock AI chatbot with Bedrock integration 3. Key Metrics Achieved\n300+ user accounts 300 employee records across 5 departments 900+ performance scores tracked 9,300+ attendance records AWS monthly cost: ~$33.14 System uptime: 99.9%+ Zero critical security vulnerabilities 4. Acceptance Status\nCurrent Status: Application deployed in cloudfront Production URL: https://d2z6tht6rq32uy.cloudfront.net 5. Next Steps\nMinor bug fixing and feature updates Conduct user acceptance testing Provide knowledge transfer and training "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week Duration: September 15 - September 21, 2025\nWeek 2 Objectives: Strengthen foundational understanding of cloud computing concepts. Learn the architecture and essential components of Amazon VPC (Virtual Private Cloud). Understand how traffic flows inside a VPC using subnets, route tables, NAT Gateway, and Internet Gateway. Gain a basic understanding of Amazon EC2, its features, and how to launch EC2 instances into a configured VPC. Practice hands-on configuration of networking components using AWS Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Learned fundamental cloud computing concepts through video lessons 09/15/2025 09/15/2025 YouTube cloud basics playlist Tuesday - Participated in CloudDay event 09/16/2025 09/16/2025 â€” Wednesday - Practice configuring VPC and its components: subnets, route tables, IGW, NAT Gateway 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/ Thursday - Study EC2 basics and learned how to launch an EC2 instance inside a VPC 09/18/2025 09/18/2025 https://000004.awsstudygroup.com/, YouTube EC2 tutorial Friday - Team meeting and decide on the final prioject 09/19/2025 09/19/2025 â€” Week 2 Achievements: Cloud Fundamentals Understood key cloud computing concepts: What cloud infrastructure is. Why elasticity and scalability help prevent websites or applications from crashing during spikes in demand. Why cloud adoption is essential in a modern tech landscape where demand increases but not every organization can afford physical infrastructure. VPC (Virtual Private Cloud) Skills Gained practical experience building a functional AWS VPC:\nCreated public and private subnets. Learned how CIDR blocks define IP ranges in a VPC. Attached an Internet Gateway (IGW) to provide internet access to public subnets. Created a NAT Gateway to allow private subnets to securely access the internet. Configured route tables to control traffic flow and enhance security. Understood the logic behind:\nPublic subnets â†’ route through IGW. Private subnets â†’ route through NAT Gateway or internal resources. Learned the security benefits of separating workloads into public and private subnets.\nEC2 (Elastic Compute Cloud) Skills Understood fundamental EC2 concepts:\nInstance types (general-purpose, compute-optimized, memory-optimized, etc.) AMIs (operating system and base configuration) SSH key pairs and authentication Security Groups for traffic control Practiced launching EC2 instances into:\nA selected VPC A specific subnet Using configured security groups and key pairs "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.3-architecture/","title":"Project Architecture","tags":[],"description":"","content":"InsightHR Architecture Deep Dive This section provides a detailed look at the InsightHR platform architecture, explaining how different AWS services work together to create a scalable, secure, and cost-effective serverless application.\nHigh-Level Architecture â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ User Browser â”‚ â”‚ (React + TypeScript) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ HTTPS â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ CloudFront CDN â”‚ â”‚ - Global edge locations â”‚ â”‚ - SSL/TLS termination â”‚ â”‚ - Caching static assets â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ S3 Static Website â”‚ â”‚ - React SPA hosting â”‚ â”‚ - Static assets (JS, CSS, images) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ REST API calls â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ API Gateway (REST) â”‚ â”‚ - Request routing â”‚ â”‚ - Cognito authorization â”‚ â”‚ - Request/response transformation â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Auth â”‚ â”‚ Employees â”‚ â”‚ Performance â”‚ â”‚ Chatbot â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Scores â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ DynamoDB â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Users â”‚ â”‚Employees â”‚ â”‚ Scores â”‚ â”‚Attendanceâ”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ AWS Bedrock â”‚ â”‚ (Claude 3 Haiku Model) â”‚ â”‚ - Natural language processing â”‚ â”‚ - Context-aware responses â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Component Breakdown 1. Frontend Layer Amazon S3 + CloudFront\nS3 Bucket: Hosts the React SPA as static files\nConfigured for static website hosting Stores HTML, JavaScript, CSS, and image assets Versioning enabled for rollback capability CloudFront Distribution: Global CDN for fast content delivery\nEdge locations worldwide for low latency SSL/TLS certificate from ACM Custom domain support (insight-hr.io.vn) Caching policies for optimal performance Origin Access Identity (OAI) for S3 security React Application\nSingle Page Application (SPA) architecture Client-side routing with React Router State management with Zustand TypeScript for type safety Tailwind CSS for styling 2. API Layer Amazon API Gateway (REST)\nEndpoints: RESTful API design\n/auth/* - Authentication endpoints /employees/* - Employee management /performance-scores/* - Performance tracking /attendance/* - Attendance management /chatbot/* - AI chatbot queries Features:\nRequest validation CORS configuration Rate limiting and throttling Request/response transformation API keys and usage plans Authorization: Cognito User Pool Authorizer\nJWT token validation Role-based access control Automatic token refresh 3. Compute Layer AWS Lambda Functions\nAuthentication Service (auth-*-handler)\nLogin with Cognito User registration Google OAuth integration Password reset workflow Token management Employee Service (employees-*-handler)\nCRUD operations for employees Department and position filtering Bulk import from CSV Search functionality Performance Scores Service (performance-scores-handler)\nScore calculation and management Quarterly performance tracking KPI aggregation Role-based data access Chatbot Service (chatbot-handler)\nNatural language query processing Context building from DynamoDB Bedrock API integration Response formatting Attendance Service (attendance-handler)\nCheck-in/check-out operations Attendance history tracking Status management Bulk operations Lambda Configuration:\nRuntime: Python 3.11 Memory: 256-512 MB Timeout: 30-60 seconds Environment variables for configuration IAM roles with least privilege 4. Data Layer Amazon DynamoDB\nTables Structure:\ninsighthr-users-dev\nPrimary Key: userId GSI: email-index Purpose: User authentication and profiles Attributes: email, name, role, employeeId, department insighthr-employees-dev\nPrimary Key: employeeId GSI: department-index Purpose: Employee master data Attributes: name, email, department, position, status insighthr-performance-scores-dev\nPrimary Key: employeeId Sort Key: period GSI: department-period-index Purpose: Quarterly performance scores Attributes: overallScore, kpiScores, calculatedAt insighthr-attendance-history-dev\nPrimary Key: employeeId Sort Key: date GSI: date-index, department-date-index Purpose: Check-in/check-out history Attributes: checkInTime, checkOutTime, status DynamoDB Features:\nOn-demand billing mode Point-in-time recovery Encryption at rest Global secondary indexes for efficient queries TTL for automatic data expiration (if needed) 5. Authentication Layer Amazon Cognito User Pools\nUser Management:\nEmail/password authentication Google OAuth integration User attributes (name, role, department) Password policies and MFA support Token Management:\nJWT tokens (ID, Access, Refresh) Token expiration and refresh Custom claims for roles Security Features:\nPassword complexity requirements Account recovery workflows User verification Brute force protection 6. AI/ML Layer Amazon Bedrock (Claude 3 Haiku)\nCapabilities:\nNatural language understanding Context-aware responses Data querying and analysis Conversational interface Integration:\nInvoked from Lambda function Context built from DynamoDB data Role-based data filtering Response formatting and validation Cost Optimization:\nHaiku model for cost-effectiveness Efficient prompt engineering Response caching where applicable 7. Monitoring Layer Amazon CloudWatch\nLogs:\nLambda function logs API Gateway access logs Error tracking and debugging Metrics:\nAPI request counts Lambda invocations and duration DynamoDB read/write capacity Error rates and latency Alarms:\nHigh error rate alerts Performance degradation Cost threshold warnings CloudWatch Synthetics Canaries:\nLogin flow testing Dashboard availability Chatbot functionality Performance score calculations Data Flow Examples User Login Flow 1. User enters credentials in React app 2. React app calls API Gateway /auth/login 3. API Gateway routes to auth-login-handler Lambda 4. Lambda authenticates with Cognito 5. Cognito returns JWT tokens 6. Lambda stores user session in DynamoDB 7. Tokens returned to React app 8. React app stores tokens in localStorage 9. Subsequent requests include JWT in Authorization header Employee Query Flow 1. User requests employee list in React app 2. React app calls API Gateway /employees with filters 3. API Gateway validates JWT token with Cognito 4. Request routed to employees-handler Lambda 5. Lambda queries DynamoDB employees table 6. Results filtered based on user role 7. Data returned to React app 8. React app displays employees in table Chatbot Query Flow 1. User types question in chatbot interface 2. React app calls API Gateway /chatbot/query 3. API Gateway validates JWT and routes to chatbot-handler 4. Lambda retrieves relevant data from DynamoDB 5. Lambda builds context and calls Bedrock API 6. Bedrock processes query with Claude 3 Haiku 7. Response formatted and returned to React app 8. React app displays answer in chat interface Security Architecture Defense in Depth:\nNetwork Security:\nHTTPS only (enforced by CloudFront) API Gateway with AWS WAF (optional) VPC endpoints for private connectivity (optional) Authentication \u0026amp; Authorization:\nCognito for user authentication JWT tokens for API authorization Role-based access control (RBAC) Least privilege IAM roles Data Security:\nEncryption at rest (DynamoDB, S3) Encryption in transit (TLS 1.2+) Secure credential management No hardcoded secrets Application Security:\nInput validation SQL injection prevention (NoSQL) XSS protection CORS configuration Scalability \u0026amp; Performance Auto-Scaling:\nLambda: Automatic concurrent execution scaling DynamoDB: On-demand capacity mode CloudFront: Global edge network API Gateway: Automatic request handling Performance Optimization:\nCloudFront caching for static assets DynamoDB GSIs for efficient queries Lambda function optimization API response caching High Availability:\nMulti-AZ deployment (automatic) CloudFront global distribution DynamoDB replication Lambda fault tolerance Cost Optimization Strategies:\nOn-demand pricing for variable workloads Lambda free tier utilization CloudFront caching to reduce origin requests DynamoDB query optimization Bedrock Haiku model for cost-effectiveness Estimated Monthly Costs (Development):\nDynamoDB: $0.50 Lambda: Free tier S3 + CloudFront: $1-2 API Gateway: $0.10 Bedrock: $0.0004 per query Total: $2-5/month Next Steps Now that you understand the architecture, let\u0026rsquo;s proceed to Setup AWS Environment to start building the platform.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the blogs you have translated. For example:\nBlog 1 - Meet the AWS News Blog Team! This blog introduces the team behind the AWS News Blog following Jeff Barrâ€™s retirement. The team consists of 11 bloggers from multiple continents, continuing the mission of delivering accurate, clear, and reliable AWS product launch updates. The article reiterates Jeff Barrâ€™s final message and affirms that the blog will continue to grow strongly. It also presents each author â€” from the new leader Channy Yun to bloggers like Danilo Poccia, SÃ©bastien Stormacq, Veliswa Boya, Donnie Prakoso, and others â€” each sharing their roles, expertise, and passion for delivering high-quality content to AWS customers.\nBlog 2 - Managing AWS Config Rules with Remediation using AWS Config Conformance Pack This blog explains how to implement a compliance evaluation and automated remediation mechanism in an AWS environment using AWS Config, conformance packs, AWS Lambda, and AWS Systems Manager. You will learn how to set up a delegated administrator account, deploy a custom conformance pack, configure cross-account IAM permissions, and build automated remediation workflows for compliance rules. The article also illustrates how to detect Security Groups with overly broad inbound CIDR ranges and automatically fix them using an Automation runbook.\nBlog 3 - MasterClass Enhances Video Quality and Reduces Costs with AWS Media Services This blog describes how the MasterClass platform improves video quality and optimizes costs by using AWS Media Services such as AWS Elemental MediaConvert and Amazon CloudFront, together with the orchestration layer from Nomad Media. After migrating its entire video processing workflow to AWS, MasterClass achieved better streaming quality, lower buffering time, and reduced video costs by more than half. The article also explains how MasterClass uses the QVBR feature to improve transcoding quality, integrates the CloudFront CDN for more efficient content delivery, and optimizes token validati\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week Duration: September 22 - September 28, 2025\nWeek 3 Objectives: Understand how synthetic (AI-generated) data works and its limitations in ML training Learn the process of assessing data quality: distribution checks, feature consistency, logical behavior Re-generate synthetic data with improved structure and realistic patterns Receive mentor feedback and refine dataset creation strategy Gain hands-on experience with Amazon S3 fundamentals and core operations Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Synthetic Data Generation: + Generate initial synthetic dataset for the ML model + Study synthetic generation methods (prompt-based, schema-based) + Ensure feature alignment with actual model requirements 09/22/2025 09/22/2025 Project Dataset Docs Tuesday-Wednesday - Data Quality Assessment: + Check distribution of each feature + Detect unrealistic patterns and missing logical relationships + Compare synthetic vs. real-world expectations + Regenerate dataset 09/23/2025 09/24/2025 Data Validation Guidelines Thursday - Mentor Consultation: + Discuss issues with synthetic-only data + Learn hybrid-data strategy + Learn how to avoid uniform patterns and low variance + Plan improved data generation pipeline 09/25/2025 09/25/2025 Mentor Session Notes Friday - Amazon S3 Fundamentals: + Learn S3 concepts: buckets, objects, regions + Study storage classes and cost impact + Practice uploading/downloading files + Configure S3 versioning and lifecycle rules 09/26/2025 09/26/2025 https://000057.awsstudygroup.com/ Week 3 Achievements: Understanding Synthetic Data Limitations:\nLearned why synthetic data cannot serve as the only training source Recognized low variance due to single-model generation Identified unrealistic data behaviors and missing edge cases Understood overfitting risks from overly clean synthetic distributions Improved Data Strategy After Mentor Feedback:\nPlanned hybrid data generation using multiple AI models Added small real data samples to improve realism Incorporated controlled randomness to mimic natural variability Applied validation through distribution checks, correlation checks, and logical consistency tests Amazon S3 Skills:\nGained strong understanding of S3 object storage concepts Learned differences in bucket-level vs. object-level permissions Studied S3 storage classes: Standard, Standard-IA, One Zone-IA, Glacier tiers Practiced creating buckets, uploading objects, enabling versioning, and configuring lifecycle rules Learned S3â€™s role in ML pipelines, static hosting, backups, and integration with Lambda, EC2, and Athena Technical Competencies:\nData generation and validation workflow Hybrid syntheticâ€“real data design Hands-on AWS S3 operations and configuration Improved understanding of data quality and ML model robustness Readiness for Next Steps:\nEstablished strong foundation in data quality assessment Built a refined strategy for future dataset generation Prepared S3 storage structure for upcoming project datasets Ready to proceed to Week 4 with improved ML and AWS fundamentals "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/4-eventparticipated/4.2-event2/","title":"Workshop: Data Science on AWS","tags":[],"description":"","content":"Workshop: Data Science on AWS Location: FPT University HCM Campus\nDate: Thursday, October 16, 2025\nEvent Objectives Explore the entire journey of building a modern Data Science system, from theory to practice Master the end-to-end Data Science workflow on AWS, including storage, processing, and model deployment Gain practical experience with real datasets (IMDb) and applied models (Sentiment Analysis) Analyze trade-offs between Cloud and On-premise infrastructure in terms of cost and performance Speaker List VÄƒn HoÃ ng Kha Cloud Solutions Architect, AWS Community Builder Báº¡ch DoÃ£n VÆ°Æ¡ng Cloud DevOps Engineer, AWS Community Builder Key Content Highlights 1. Role of Cloud in Data Science and Workflow Overview Importance of Cloud: Introduction how cloud services like AWS could help developer simplify their data processing pipeline and at the same time help them save cost Data Science Workflow on AWS: Storage: Use Amazon S3 as the core data lake platform ETL/Processing: Use AWS Glue for serverless data integration tasks Modeling: Leverage Amazon SageMaker as the central hub for building, training, and deploying models Overview of AWS\u0026rsquo;s extensive AI/ML ecosystem, including AI services, ML services, and infrastructure 2. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Scenario: Process and clean raw data from the IMDb dataset Technique: Demonstrate effective feature engineering and data preparation methods. The workshop highlights different approaches, from low-code options like SageMaker Canvas to code-first methods using Numpy/Pandas Demo 2: Sentiment Analysis with SageMaker: Scenario: Train and deploy a machine learning model for sentiment analysis from text data Process: Illustrate the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; cycle in SageMaker Studio. The session also covers the \u0026ldquo;Bring Your Own Model (BYOM)\u0026rdquo; concept, showcasing flexibility with frameworks like TensorFlow and PyTorch 3. Strategic Discussion Cloud versus On-Premise: In-depth discussion on cost optimization and performance metrics. Content highlights how cloud elasticity enables testing heavy workloads without large upfront hardware investment for on-premise infrastructure Small Project Guide: Introduction to a project designed after the workshop to reinforce learned skills What I Learned Technical Workflow Unified Workflow: With wide range of AI services provided by cloud platform, the process of building AI application is as simple as ever, reduce the complexity of abstract theory Tool Selection: Learn to choose the right tools suitable for different scenario (Amazon Transcribe, Amazon Textract) and the pricing of each service to manage a balance project Industry Application Application to Work Apply AWS Glue: Propose converting local ETL scripts to AWS Glue for serverless data processing automation on larger datasets Deploy SageMaker: Migrate experimental models from local Jupyter notebooks to SageMaker Studio to standardize training and deployment workflows Implement Project: Execute the small project proposed after the workshop to solidify understanding of IMDb data processing workflow Event Experience The workshop \u0026ldquo;Data Science on AWS\u0026rdquo; show how Cloud platform has bring the process of building AI application closer to both business and users.\nWith the rise of AI, businesses have to adapt quickly to the trend. However, the task of carrying AI application are both compplicated and requires high compute power , with services from Cloud provider like AWS, the process of building AI application has never been easier. Services like Textract, Transcribe, Bedrock,\u0026hellip; have made it easier for business or even personal user to build their own AI application with reduced cost and more importantly, reduced complexity Some photos from the event Overall, the workshop provided a comprehensive Data Science framework, emphasizing the importance of AWS managed tools for achieving flexibility, scalability, and cost optimization.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.4-setup-aws/","title":"Setup AWS Environment","tags":[],"description":"","content":"Setting Up AWS Environment This section guides you through configuring your AWS environment for the InsightHR platform, including IAM roles, policies, and initial service configuration.\nOverview We\u0026rsquo;ll set up:\nIAM roles for Lambda functions IAM policies for service access AWS region configuration Service quotas verification Bedrock model access Step 1: Configure AWS Region Set your default region to Singapore (ap-southeast-1):\n# Configure AWS CLI aws configure set region ap-southeast-1 # Verify configuration aws configure get region Why Singapore?\nAll required services available Good latency for Southeast Asia Bedrock Claude 3 Haiku available Step 2: Create IAM Role for Lambda Lambda functions need an execution role to access AWS services.\nCreate Trust Policy lambda-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the Role # Create IAM role aws iam create-role \\ --role-name insighthr-lambda-role \\ --assume-role-policy-document file://lambda-trust-policy.json \\ --description \u0026#34;Execution role for InsightHR Lambda functions\u0026#34; Step 3: Create IAM Policies DynamoDB Access Policy dynamodb-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/insighthr-*\u0026#34; ] } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-dynamodb-policy \\ --policy-document file://dynamodb-policy.json Cognito Access Policy cognito-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:cognito-idp:ap-southeast-1:*:userpool/*\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-cognito-policy \\ --policy-document file://cognito-policy.json Bedrock Access Policy bedrock-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-bedrock-policy \\ --policy-document file://bedrock-policy.json Step 4: Attach Policies to Role # Get AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Attach AWS managed policy for Lambda basic execution aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # Attach custom policies aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-dynamodb-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-cognito-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-bedrock-policy Step 5: Create IAM Role for API Gateway API Gateway needs a role to write logs to CloudWatch.\napi-gateway-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the role:\naws iam create-role \\ --role-name insighthr-apigateway-role \\ --assume-role-policy-document file://api-gateway-trust-policy.json # Attach CloudWatch logs policy aws iam attach-role-policy \\ --role-name insighthr-apigateway-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs Step 6: Enable Bedrock Model Access Request access to Claude 3 Haiku model:\nUsing AWS Console:\nNavigate to Amazon Bedrock Click \u0026ldquo;Model access\u0026rdquo; in left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box Click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant) Verify Access:\naws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude-3-haiku`)].modelId\u0026#39; Step 7: Verify Service Quotas Check service limits for your account:\n# Lambda concurrent executions aws service-quotas get-service-quota \\ --service-code lambda \\ --quota-code L-B99A9384 \\ --region ap-southeast-1 # DynamoDB tables aws service-quotas get-service-quota \\ --service-code dynamodb \\ --quota-code L-F98FE922 \\ --region ap-southeast-1 # API Gateway requests per second aws service-quotas get-service-quota \\ --service-code apigateway \\ --quota-code L-8A5B8E43 \\ --region ap-southeast-1 Step 8: Create S3 Bucket for Deployment Artifacts # Create bucket for Lambda deployment packages aws s3 mb s3://insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --region ap-southeast-1 # Enable versioning aws s3api put-bucket-versioning \\ --bucket insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --versioning-configuration Status=Enabled Step 9: Set Up CloudWatch Log Groups Pre-create log groups for Lambda functions:\n# Create log groups LOG_GROUPS=( \u0026#34;/aws/lambda/insighthr-auth-login-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-register-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-google-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-employees-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-performance-scores-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-chatbot-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-attendance-handler\u0026#34; ) for log_group in \u0026#34;${LOG_GROUPS[@]}\u0026#34;; do aws logs create-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 # Set retention to 7 days aws logs put-retention-policy \\ --log-group-name $log_group \\ --retention-in-days 7 \\ --region ap-southeast-1 done Step 10: Configure Environment Variables Create a configuration file for environment variables:\nconfig.env:\n# AWS Configuration export AWS_REGION=ap-southeast-1 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # DynamoDB Tables export USERS_TABLE=insighthr-users-dev export EMPLOYEES_TABLE=insighthr-employees-dev export PERFORMANCE_SCORES_TABLE=insighthr-performance-scores-dev export ATTENDANCE_HISTORY_TABLE=insighthr-attendance-history-dev # Cognito (will be set after Cognito setup) export USER_POOL_ID= export CLIENT_ID= # Bedrock export BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0 export BEDROCK_REGION=ap-southeast-1 # Lambda Role ARN export LAMBDA_ROLE_ARN=arn:aws:iam::${AWS_ACCOUNT_ID}:role/insighthr-lambda-role Load the configuration:\nsource config.env Verification Checklist Verify your AWS environment is ready:\n# Check IAM role exists aws iam get-role --role-name insighthr-lambda-role # Check policies are attached aws iam list-attached-role-policies --role-name insighthr-lambda-role # Check Bedrock access aws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude`)].modelId\u0026#39; # Check S3 bucket exists aws s3 ls s3://insighthr-deployment-artifacts-${ACCOUNT_ID} # Check CloudWatch log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 IAM Setup Summary Resource Purpose Policies Attached insighthr-lambda-role Lambda execution AWSLambdaBasicExecutionRole, DynamoDB, Cognito, Bedrock insighthr-apigateway-role API Gateway logging AmazonAPIGatewayPushToCloudWatchLogs insighthr-dynamodb-policy DynamoDB access Custom policy for table operations insighthr-cognito-policy Cognito access Custom policy for user management insighthr-bedrock-policy Bedrock access Custom policy for model invocation Security Best Practices âœ… Least Privilege: Policies grant only necessary permissions âœ… Resource Restrictions: Policies limited to insighthr-* resources âœ… Separate Roles: Different roles for different services âœ… CloudWatch Logging: All Lambda functions log to CloudWatch âœ… Versioning: S3 bucket versioning enabled for artifacts\nTroubleshooting IAM Role Creation Fails:\nCheck IAM permissions Verify trust policy syntax Ensure role name is unique Policy Attachment Fails:\nVerify policy ARN is correct Check role exists Ensure you have iam:AttachRolePolicy permission Bedrock Access Denied:\nRequest model access in Bedrock console Wait for approval (usually instant for Haiku) Verify region supports Bedrock Service Quota Issues:\nRequest quota increase in Service Quotas console Use different region if needed Contact AWS support for urgent increases Next Steps With the AWS environment configured, proceed to Database Setup to create DynamoDB tables.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week Duration: September 29 - October 5, 2025\nWeek 4 Objectives: Prepare final dataset for ML model training Learn AWS Lambda fundamentals and understand how it reduces cost in serverless workflows Clean and finalize the dataset for model training Study CloudTrail and CloudWatch, including their differences and use cases Learn the structure and working principles of the Transformer architecture Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - AWS Lambda Exploration: + Understand Lambda functions and event-driven compute + Identify Lambda use cases for cost reduction in final project 09/29/2025 09/29/2025 AWS Lambda Docs Tuesday - Final Data Cleaning: + Clean dataset + Remove noise/outliers + Ensure consistency before training 09/30/2025 09/30/2025 Project Dataset Guidelines Wednesday-Thursday - CloudTrail \u0026amp; CloudWatch: + Learn AWS CloudTrail auditing features + Learn CloudWatch metrics/logs/alarms + Understand the difference between them 10/01/2025 10/02/2025 https://www.youtube.com/watch?v=S5X0PnBwp9I Friday - Transformer Model Study: + Learn Transformer architecture + Study attention mechanisms and encoder-decoder pipeline 10/03/2025 10/03/2025 https://www.youtube.com/watch?v=biveB0gOlak\u0026t=5779s Week 4 Achievements: Final Dataset Preparation:\nProduced a realistic, high-quality synthetic dataset Ensured normal distribution across key features Removed unnatural spikes and inconsistent patterns Final dataset ready for model training AWS Lambda Knowledge:\nUnderstood Lambda event-driven compute model Learned how Lambda reduces cost compared to always-on EC2 Identified use cases for integrating Lambda into the final project CloudTrail \u0026amp; CloudWatch Expertise:\nLearned CloudTrailâ€™s role in tracking account activity and auditing Learned CloudWatchâ€™s role in monitoring metrics and logs Clearly understood the differences and how both services complement each other Transformer Architecture Fundamentals:\nStudied self-attention and multi-head attention Understood encoder/decoder flow and positional encoding Gained insight into why Transformers outperform RNNs "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" âš ï¸ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3â€¦, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the eventâ€™s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nVietnam Cloud Day 2025 Event Name: Vietnam Cloud Day 2025 â€“ Ho Chi Minh City Connect Edition for Builders (GenAI \u0026amp; Data Track)\nDate \u0026amp; Time: 09:00, Thursday, September 18, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nData Science on AWS Event Name: Data Science on AWS Workshop\nDate \u0026amp; Time: 09:00, Thursday, October 16, 2025\nLocation: FPT University, Ho Chi Minh City Campus\nRole: Attendee\nCloudThinker: Agentic AI \u0026amp; Orchestration on AWS Event Name: CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\nDate \u0026amp; Time: 09:00, Friday, December 5, 2025\nLocation: AWS Event Hall, Level 26, Bitexco Tower, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.5-database-setup/","title":"Database Setup (DynamoDB)","tags":[],"description":"","content":"DynamoDB Database Setup In this section, you\u0026rsquo;ll create and configure the DynamoDB tables required for the InsightHR platform.\nOverview InsightHR uses Amazon DynamoDB, a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. We\u0026rsquo;ll create 4 main tables:\nUsers Table - Authentication and user profiles Employees Table - Employee master data Performance Scores Table - Quarterly performance tracking Attendance History Table - Check-in/check-out records Table Design Principles DynamoDB Best Practices:\nUse partition keys for even data distribution Add sort keys for range queries Create GSIs for alternate query patterns Denormalize data for read performance Use on-demand billing for variable workloads Step 1: Create Users Table Table Configuration:\nTable Name: insighthr-users-dev Partition Key: userId (String) Billing Mode: On-demand Using AWS Console:\nNavigate to DynamoDB in AWS Console Click \u0026ldquo;Create table\u0026rdquo; Enter table name: insighthr-users-dev Partition key: userId (String) Table settings: Use default settings Billing mode: On-demand Click \u0026ldquo;Create table\u0026rdquo; Add Global Secondary Index:\nSelect the table Go to \u0026ldquo;Indexes\u0026rdquo; tab Click \u0026ldquo;Create index\u0026rdquo; Index name: email-index Partition key: email (String) Projected attributes: All Click \u0026ldquo;Create index\u0026rdquo; Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=userId,AttributeType=S \\ AttributeName=email,AttributeType=S \\ --key-schema \\ AttributeName=userId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI aws dynamodb update-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=email,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;email-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;email\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;userId\u0026#34;: \u0026#34;user-123\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Admin User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 2: Create Employees Table Table Configuration:\nTable Name: insighthr-employees-dev Partition Key: employeeId (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department queries aws dynamodb update-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;hireDate\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;managerId\u0026#34;: \u0026#34;DEV-000\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 3: Create Performance Scores Table Table Configuration:\nTable Name: insighthr-performance-scores-dev Partition Key: employeeId (String) Sort Key: period (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=period,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=period,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department-period queries aws dynamodb update-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=period,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-period-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;period\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;scoreId\u0026#34;: \u0026#34;score-123\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;period\u0026#34;: \u0026#34;2025-Q1\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;overallScore\u0026#34;: 85.5, \u0026#34;kpiScores\u0026#34;: { \u0026#34;KPI\u0026#34;: 85.0, \u0026#34;completed_task\u0026#34;: 88.0, \u0026#34;feedback_360\u0026#34;: 83.5 }, \u0026#34;calculatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 4: Create Attendance History Table Table Configuration:\nTable Name: insighthr-attendance-history-dev Partition Key: employeeId (String) Sort Key: date (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=date,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=date,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 # Create GSI for department-date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-12-09\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;checkInTime\u0026#34;: \u0026#34;09:00:00\u0026#34;, \u0026#34;checkOutTime\u0026#34;: \u0026#34;18:00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;present\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;On time\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T09:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T18:00:00Z\u0026#34; } Step 5: Verify Table Creation Using AWS Console:\nNavigate to DynamoDB Check that all 4 tables are listed Verify each table status is \u0026ldquo;Active\u0026rdquo; Check GSIs are created and active Using AWS CLI:\n# List all tables aws dynamodb list-tables --region ap-southeast-1 # Describe specific table aws dynamodb describe-table \\ --table-name insighthr-users-dev \\ --region ap-southeast-1 Step 6: Load Sample Data (Optional) For testing purposes, you can load sample data into the tables.\nCreate sample data file (sample-users.json):\n{ \u0026#34;insighthr-users-dev\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;userId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user-001\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;admin@example.com\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin User\u0026#34;}, \u0026#34;role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}, \u0026#34;employeeId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV-001\u0026#34;}, \u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV\u0026#34;}, \u0026#34;isActive\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;createdAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;}, \u0026#34;updatedAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;} } } } ] } Load data:\naws dynamodb batch-write-item \\ --request-items file://sample-users.json \\ --region ap-southeast-1 Database Configuration Summary Table Partition Key Sort Key GSIs Purpose insighthr-users-dev userId - email-index User authentication insighthr-employees-dev employeeId - department-index Employee data insighthr-performance-scores-dev employeeId period department-period-index Performance tracking insighthr-attendance-history-dev employeeId date date-index, department-date-index Attendance records Best Practices Implemented âœ… Partition Key Design: Even distribution of data âœ… Sort Keys: Enable range queries for time-series data âœ… GSIs: Support alternate query patterns âœ… On-Demand Billing: Cost-effective for variable workloads âœ… Naming Convention: Consistent table naming with environment suffix\nTroubleshooting Table Creation Fails:\nCheck IAM permissions for DynamoDB Verify region is correct Ensure table name doesn\u0026rsquo;t already exist GSI Creation Fails:\nWait for table to be ACTIVE before creating GSI Check attribute definitions match Verify IAM permissions High Costs:\nUse on-demand billing for development Monitor read/write capacity units Optimize query patterns Next Steps With the database tables created, proceed to Authentication Service to set up Cognito and authentication Lambda functions.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/","title":"Workshop","tags":[],"description":"","content":"InsightHR - Serverless HR Automation Platform Workshop Overview InsightHR is a modern, fully serverless HR automation platform built on AWS that demonstrates best practices for cloud-native application development. This workshop guides you through building and deploying a complete production-ready application using AWS services.\nWhat You\u0026rsquo;ll Build A comprehensive HR management system featuring:\nEmployee Management: Full CRUD operations with advanced filtering Performance Tracking: Quarterly performance scores and KPI management Attendance System: Real-time check-in/check-out with history tracking AI Chatbot: Natural language queries powered by AWS Bedrock (Claude 3) Dashboard Analytics: Interactive performance visualization Role-Based Access Control: Admin, Manager, and Employee roles Authentication: Email/password and Google OAuth via AWS Cognito Architecture Highlights âœ… 100% Serverless - No EC2 instances to manage âœ… Scalable - Auto-scales with demand âœ… Cost-Effective - Pay only for what you use âœ… Secure - Built-in security with Cognito and IAM âœ… Modern Stack - React + TypeScript + Python âœ… Production-Ready - CloudWatch monitoring and custom domain AWS Services Used Frontend: S3 + CloudFront + Route53 Backend: Lambda + API Gateway + DynamoDB Authentication: Cognito User Pools AI/ML: Amazon Bedrock (Claude 3 Haiku) Monitoring: CloudWatch + Synthetics Canaries Security: IAM + ACM (SSL Certificates) Workshop Content Workshop Overview Prerequisites Project Architecture Setup AWS Environment Database Setup (DynamoDB) Authentication Service Backend Services Frontend Development Deployment Testing \u0026amp; Monitoring Cleanup Learning Outcomes By completing this workshop, you will learn:\nHow to design and implement serverless architectures Best practices for AWS Lambda and API Gateway DynamoDB data modeling and optimization AWS Cognito authentication flows Integration with AWS Bedrock for AI capabilities CloudFront CDN configuration Infrastructure as Code principles Production deployment strategies Cost optimization techniques Prerequisites AWS Account with appropriate permissions Basic knowledge of JavaScript/TypeScript and Python Familiarity with React framework Understanding of REST APIs AWS CLI installed and configured Estimated Time Full Workshop: 4-6 hours Core Features Only: 2-3 hours Cost Estimate Running this workshop will incur minimal AWS costs:\nDynamoDB: ~$0.50/month (on-demand pricing) Lambda: Free tier covers most usage S3 + CloudFront: ~$1-2/month API Gateway: ~$0.10/month Bedrock: ~$0.0004 per query Total: ~$2-5/month for development Remember to clean up resources after completing the workshop to avoid ongoing charges.\nSupport For questions or issues during the workshop:\nCheck the troubleshooting sections in each module Review AWS documentation links provided Refer to the GitHub repository for code samples Let\u0026rsquo;s get started! ğŸš€\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week Duration: October 6 - October 12, 2025\nWeek 5 Objectives: Test ML models to determine which architecture best fits the dataset (XGBoost vs Neural Network) Understand why XGBoost consistently excels for tabular data Learn NoSQL and DynamoDB fundamentals and how they support scalable applications Build a baseline XGBoost model for the final project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Model Architecture Testing: + Compare XGBoost and Neural Networks + Evaluate training time, overfitting behavior, and validation performance 10/06/2025 10/06/2025 ML Model Comparison Notes Tuesday - Why XGBoost Excels: + Study tree-based advantages + Read research on XGBoost vs deep learning for tabular data 10/07/2025 10/07/2025 https://arxiv.org/pdf/2207.08815 Wednesday-Thursday - DynamoDB \u0026amp; NoSQL Discovery: + Learn NoSQL fundamentals + Understand DynamoDB\u0026rsquo;s scalability model + Learn partition keys, sort keys, RCU/WCU, and autoscaling strategies 10/08/2025 10/09/2025 https://www.youtube.com/watch?v=0buKQHokLK8 Friday - Baseline XGBoost Model: + Build baseline XGBoost model + Perform hyperparameter tuning + Document metrics for later model comparison 10/10/2025 10/10/2025 XGBoost Documentation Week 5 Achievements: Model Comparison Completed:\nTested XGBoost vs Neural Network performance Identified the best architecture for the dataset based on accuracy, consistency, and training speed Understanding XGBoostâ€™s Strength:\nLearned why tree-based models outperform deep learning on tabular data Understood feature interaction modeling, natural handling of heterogeneous data, and boosting advantages NoSQL \u0026amp; DynamoDB Knowledge:\nStudied NoSQL principles and scalability benefits Learned DynamoDB core concepts: partitioning, RCU/WCU, on-demand mode, and global tables Understood when NoSQL is preferred over relational databases Baseline XGBoost Model Completed:\nBuilt and tested a baseline model Tuned hyperparameters and recorded evaluation metrics Established a foundation for future training and model optimization Technical Competencies:\nModel comparison and selection Tree-based model theory NoSQL database design Hands-on DynamoDB concepts Baseline ML model development workflow "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.6-authentication/","title":"Authentication Service","tags":[],"description":"","content":"Authentication Service Setup This section covers setting up AWS Cognito for user authentication and implementing authentication Lambda functions.\nOverview The authentication system includes:\nAWS Cognito User Pool - User management and authentication Login Handler - Email/password authentication Registration Handler - New user signup Google OAuth Handler - Social login integration Password Reset - Password recovery workflow Step 1: Create Cognito User Pool Using AWS Console Navigate to Amazon Cognito\nClick \u0026ldquo;Create user pool\u0026rdquo;\nConfigure sign-in experience:\nSign-in options: Email User name requirements: Allow email addresses Click \u0026ldquo;Next\u0026rdquo; Configure security requirements:\nPassword policy: Cognito defaults Multi-factor authentication: Optional User account recovery: Email only Click \u0026ldquo;Next\u0026rdquo; Configure sign-up experience:\nSelf-registration: Enabled Required attributes: name, email Click \u0026ldquo;Next\u0026rdquo; Configure message delivery:\nEmail provider: Send email with Cognito FROM email address: no-reply@verificationemail.com Click \u0026ldquo;Next\u0026rdquo; Integrate your app:\nUser pool name: insighthr-user-pool App client name: insighthr-web-client Client secret: Don\u0026rsquo;t generate Authentication flows: ALLOW_USER_PASSWORD_AUTH, ALLOW_REFRESH_TOKEN_AUTH Click \u0026ldquo;Next\u0026rdquo; Review and create\nUsing AWS CLI # Create user pool aws cognito-idp create-user-pool \\ --pool-name insighthr-user-pool \\ --policies \u0026#39;{ \u0026#34;PasswordPolicy\u0026#34;: { \u0026#34;MinimumLength\u0026#34;: 8, \u0026#34;RequireUppercase\u0026#34;: true, \u0026#34;RequireLowercase\u0026#34;: true, \u0026#34;RequireNumbers\u0026#34;: true, \u0026#34;RequireSymbols\u0026#34;: false } }\u0026#39; \\ --auto-verified-attributes email \\ --username-attributes email \\ --schema \u0026#39;[ { \u0026#34;Name\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true }, { \u0026#34;Name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true } ]\u0026#39; \\ --region ap-southeast-1 # Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;User Pool ID: $USER_POOL_ID\u0026#34; Create App Client # Create app client aws cognito-idp create-user-pool-client \\ --user-pool-id $USER_POOL_ID \\ --client-name insighthr-web-client \\ --no-generate-secret \\ --explicit-auth-flows ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH \\ --region ap-southeast-1 # Get client ID CLIENT_ID=$(aws cognito-idp list-user-pool-clients \\ --user-pool-id $USER_POOL_ID \\ --query \u0026#34;UserPoolClients[?ClientName==\u0026#39;insighthr-web-client\u0026#39;].ClientId\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;Client ID: $CLIENT_ID\u0026#34; Step 2: Configure Cognito for Development For development, disable email verification requirement:\n# Update user pool to auto-verify emails aws cognito-idp update-user-pool \\ --user-pool-id $USER_POOL_ID \\ --auto-verified-attributes email \\ --region ap-southeast-1 Step 3: Create Login Lambda Function Create Function Code auth_login_handler.py:\nimport json import boto3 import os from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] # Authenticate with Cognito response = cognito_client.admin_initiate_auth( UserPoolId=USER_POOL_ID, ClientId=CLIENT_ID, AuthFlow=\u0026#39;ADMIN_NO_SRP_AUTH\u0026#39;, AuthParameters={ \u0026#39;USERNAME\u0026#39;: email, \u0026#39;PASSWORD\u0026#39;: password } ) # Get user attributes user_response = cognito_client.admin_get_user( UserPoolId=USER_POOL_ID, Username=email ) # Extract user info user_attributes = {attr[\u0026#39;Name\u0026#39;]: attr[\u0026#39;Value\u0026#39;] for attr in user_response[\u0026#39;UserAttributes\u0026#39;]} # Get user from DynamoDB table = dynamodb.Table(USERS_TABLE) db_response = table.get_item( Key={\u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;]} ) user_data = db_response.get(\u0026#39;Item\u0026#39;, {}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;tokens\u0026#39;: { \u0026#39;accessToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;AccessToken\u0026#39;], \u0026#39;idToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;IdToken\u0026#39;], \u0026#39;refreshToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;RefreshToken\u0026#39;] }, \u0026#39;user\u0026#39;: { \u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;], \u0026#39;email\u0026#39;: user_attributes[\u0026#39;email\u0026#39;], \u0026#39;name\u0026#39;: user_attributes.get(\u0026#39;name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;role\u0026#39;: user_data.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;), \u0026#39;department\u0026#39;: user_data.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;employeeId\u0026#39;: user_data.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) } }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;NotAuthorizedException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 401, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Invalid credentials\u0026#39;}) } elif error_code == \u0026#39;UserNotFoundException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User not found\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Create requirements.txt boto3\u0026gt;=1.26.0 Package and Deploy # Create deployment package mkdir -p lambda/auth/package cd lambda/auth pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy Lambda function aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 4: Create Registration Lambda Function auth_register_handler.py:\nimport json import boto3 import os import uuid from datetime import datetime from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] name = body[\u0026#39;name\u0026#39;] role = body.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;) department = body.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;) employee_id = body.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) # Create user in Cognito cognito_response = cognito_client.sign_up( ClientId=CLIENT_ID, Username=email, Password=password, UserAttributes=[ {\u0026#39;Name\u0026#39;: \u0026#39;email\u0026#39;, \u0026#39;Value\u0026#39;: email}, {\u0026#39;Name\u0026#39;: \u0026#39;name\u0026#39;, \u0026#39;Value\u0026#39;: name} ] ) user_id = cognito_response[\u0026#39;UserSub\u0026#39;] # Auto-confirm user for development cognito_client.admin_confirm_sign_up( UserPoolId=USER_POOL_ID, Username=email ) # Store user in DynamoDB table = dynamodb.Table(USERS_TABLE) timestamp = datetime.utcnow().isoformat() + \u0026#39;Z\u0026#39; table.put_item( Item={ \u0026#39;userId\u0026#39;: user_id, \u0026#39;email\u0026#39;: email, \u0026#39;name\u0026#39;: name, \u0026#39;role\u0026#39;: role, \u0026#39;department\u0026#39;: department, \u0026#39;employeeId\u0026#39;: employee_id, \u0026#39;isActive\u0026#39;: True, \u0026#39;createdAt\u0026#39;: timestamp, \u0026#39;updatedAt\u0026#39;: timestamp } ) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;User registered successfully\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;UsernameExistsException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 409, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User already exists\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Deploy the registration handler:\nzip -g auth-register-handler.zip auth_register_handler.py aws lambda create-function \\ --function-name insighthr-auth-register-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_register_handler.lambda_handler \\ --zip-file fileb://auth-register-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 5: Create API Gateway Endpoints Create REST API # Create API API_ID=$(aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;API ID: $API_ID\u0026#34; Create Cognito Authorizer # Create authorizer AUTHORIZER_ID=$(aws apigateway create-authorizer \\ --rest-api-id $API_ID \\ --name insighthr-cognito-authorizer \\ --type COGNITO_USER_POOLS \\ --provider-arns arn:aws:cognito-idp:ap-southeast-1:${ACCOUNT_ID}:userpool/${USER_POOL_ID} \\ --identity-source method.request.header.Authorization \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;Authorizer ID: $AUTHORIZER_ID\u0026#34; Create /auth Resource # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Create /auth resource AUTH_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part auth \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create /auth/login resource LOGIN_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AUTH_ID \\ --path-part login \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create POST method for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --authorization-type NONE \\ --region ap-southeast-1 # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:${ACCOUNT_ID}:function:insighthr-auth-login-handler/invocations \\ --region ap-southeast-1 # Grant API Gateway permission to invoke Lambda aws lambda add-permission \\ --function-name insighthr-auth-login-handler \\ --statement-id apigateway-invoke \\ --action lambda:InvokeFunction \\ --principal apigateway.amazonaws.com \\ --source-arn \u0026#34;arn:aws:execute-api:ap-southeast-1:${ACCOUNT_ID}:${API_ID}/*/*\u0026#34; \\ --region ap-southeast-1 Step 6: Enable CORS # Enable CORS for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --authorization-type NONE \\ --region ap-southeast-1 aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --type MOCK \\ --request-templates \u0026#39;{\u0026#34;application/json\u0026#34;: \u0026#34;{\\\u0026#34;statusCode\\\u0026#34;: 200}\u0026#34;}\u0026#39; \\ --region ap-southeast-1 aws apigateway put-method-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: true }\u0026#39; \\ --region ap-southeast-1 aws apigateway put-integration-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;POST,OPTIONS\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;*\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34; }\u0026#39; \\ --region ap-southeast-1 Step 7: Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; \\ --region ap-southeast-1 # Get API endpoint echo \u0026#34;API Endpoint: https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 8: Test Authentication Test Registration curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Employee\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34; }\u0026#39; Test Login curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34; }\u0026#39; Authentication Flow Summary 1. User submits credentials â†’ Frontend 2. Frontend calls /auth/login â†’ API Gateway 3. API Gateway routes to Lambda â†’ auth-login-handler 4. Lambda authenticates with Cognito â†’ Cognito User Pool 5. Cognito returns JWT tokens â†’ Lambda 6. Lambda retrieves user data â†’ DynamoDB 7. Lambda returns tokens + user data â†’ Frontend 8. Frontend stores tokens â†’ localStorage 9. Subsequent requests include JWT â†’ Authorization header Security Best Practices âœ… Password Policy: Strong password requirements âœ… JWT Tokens: Short-lived access tokens âœ… Refresh Tokens: Long-lived for token renewal âœ… HTTPS Only: All communication encrypted âœ… CORS: Properly configured for frontend domain âœ… No Secrets: Client doesn\u0026rsquo;t use client secret\nTroubleshooting Cognito Authentication Fails:\nVerify user pool ID and client ID Check password meets requirements Ensure user is confirmed Lambda Permission Denied:\nCheck IAM role has Cognito permissions Verify Lambda execution role attached CORS Errors:\nEnable CORS on API Gateway Check Access-Control-Allow-Origin header Verify OPTIONS method configured Next Steps With authentication configured, proceed to Backend Services to implement employee, performance, and chatbot APIs.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week Duration: October 13 - October 19, 2025\nWeek 6 Objectives: Fine-tune and optimize the XGBoost model for the final dataset Complete model training and validation in the local environment Participate in AWS AI/ML workshop to explore cloud-based ML tools Study AWS CloudFront and Route 53 for CDN and DNS fundamentals Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday-Tuesday - XGBoost Fine-Tuning: + Adjust hyperparameters (learning rate, depth, subsample, regularization) + Validate performance and avoid overfitting 10/13/2025 10/14/2025 XGBoost Documentation Wednesday - Local Training Completion: + Train final XGBoost model + Validate preprocessing pipeline + Save model artifacts 10/15/2025 10/15/2025 Local Development Notebook Thursday - AWS AI/ML Workshop: + Attend sessions + Explore Textract, Comprehend, and SageMaker + Learn ML workflow best practices in AWS 10/16/2025 10/16/2025 AWS Workshop Materials Friday - CloudFront \u0026amp; Route 53 Study: + Learn CDN caching, latency reduction + Study DNS routing, domain setup, and health checks 10/17/2025 10/17/2025 CloudFront \u0026amp; Route 53 Docs Week 6 Achievements: Final XGBoost Model Completed:\nFine-tuned hyperparameters Trained final model version Achieved best performance for the dataset Model Training Pipeline:\nCompleted training and evaluation in local environment Verified dataset preprocessing pipeline AWS AI/ML Workshop Insights:\nDiscovered AWS ML services such as Textract and Comprehend support end-to-end AI driven applications Gained knowledge on AWS ML workflows and tools CloudFront \u0026amp; Route 53 Knowledge:\nLearned CDN global distribution concepts, hÆ¡w cloudfront leverages edge location for better content delivery Understood DNS routing, domain management, and health checks "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS FCJ Workforce from September 7, 2025 to November 25, 2025, I had the opportunity to experience a professional working environment and see how professionals operate compared to school. I gained exposure to new knowledge and AWS Cloud services that helped simplify software development processes. During this period, I had the chance to build an end-to-end website using various AWS tools, which helped me understand real-world application deployment. Additionally, I learned not only technical skills but also professional behaviors and work ethics by observing and interacting with mentors, gaining insights into how to thrive in a corporate environment.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality â˜ âœ… â˜ 2 Ability to learn Ability to absorb new knowledge and learn quickly â˜ âœ… â˜ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions â˜ â˜ âœ… 4 Sense of responsibility Completing tasks on time and ensuring quality âœ… â˜ â˜ 5 Discipline Adhering to schedules, rules, and work processes â˜ âœ… â˜ 6 Progressive mindset Willingness to receive feedback and improve oneself âœ… â˜ â˜ 7 Communication Presenting ideas and reporting work clearly â˜ âœ… â˜ 8 Teamwork Working effectively with colleagues and participating in teams âœ… â˜ â˜ 9 Professional conduct Respecting colleagues, partners, and the work environment â˜ âœ… â˜ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity âœ… â˜ â˜ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team â˜ âœ… â˜ 12 Overall General evaluation of the entire internship period âœ… â˜ â˜ Needs Improvement Improving teamwork communication skills between members Not afraid or shy to ask mentors for help Better time management Be more active on finding and learning new knowledge Reflection Overall, AWS FCJ Course has helped not only study new knowledge but also soft skills: how to ask for help from mentor, how to communicate better. And more importantly, a chance to expericen how professional workforce works so that I can apply these knowledge in the future\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.7-backend-services/","title":"Backend Services","tags":[],"description":"","content":"Backend Services Implementation This section covers implementing the core backend Lambda functions for employee management, performance tracking, attendance, and AI chatbot.\nOverview We\u0026rsquo;ll implement four main services:\nEmployee Service - CRUD operations for employee records Performance Scores Service - Quarterly performance tracking Attendance Service - Check-in/check-out management Chatbot Service - AI-powered natural language queries Employee Service The employee service handles all employee-related operations with department and position filtering.\nKey Features:\nCRUD operations (Create, Read, Update, Delete) Department filtering (DEV, QA, DAT, SEC, AI) Position filtering (Junior, Mid, Senior, Lead, Manager) Status filtering (active, inactive) Search by name or employee ID Bulk import from CSV API Endpoints:\nGET /employees - List all employees GET /employees/{id} - Get employee by ID POST /employees - Create new employee PUT /employees/{id} - Update employee DELETE /employees/{id} - Delete employee POST /employees/bulk - Bulk import from CSV Performance Scores Service Manages quarterly performance scores with automatic calculation.\nKey Features:\nAutomatic score calculation (average of KPI, completed_task, feedback_360) Department and period filtering Role-based data access (Admin: all, Manager: department, Employee: own) Denormalized employee data for performance Bulk import from CSV API Endpoints:\nGET /performance-scores - List all scores GET /performance-scores/{id}/{period} - Get specific score POST /performance-scores - Create score PUT /performance-scores/{id}/{period} - Update score DELETE /performance-scores/{id}/{period} - Delete score POST /performance-scores/bulk - Bulk import Attendance Service Tracks employee attendance with check-in/check-out functionality.\nKey Features:\nReal-time check-in/check-out Date range filtering Department filtering Status tracking (present, absent, late, half-day) Bulk operations API Endpoints:\nGET /attendance - List attendance records GET /attendance/{id}/{date} - Get specific record POST /attendance/check-in - Check in POST /attendance/check-out - Check out PUT /attendance/{id}/{date} - Update record DELETE /attendance/{id}/{date} - Delete record Chatbot Service AI-powered chatbot using AWS Bedrock (Claude 3 Haiku).\nKey Features:\nAWS Bedrock integration (Claude 3 Haiku) Natural language query processing Context building from DynamoDB Role-based data filtering Cost-effective ($0.0004 per query) Supported Queries:\nEmployee information (\u0026ldquo;Who is DEV-001?\u0026rdquo;) Performance queries (\u0026ldquo;What\u0026rsquo;s the average score for Q1 2025?\u0026rdquo;) Department statistics (\u0026ldquo;Compare DEV and QA performance\u0026rdquo;) Trend analysis (\u0026ldquo;Performance trends over quarters\u0026rdquo;) API Endpoint:\nPOST /chatbot/query Implementation Pattern All services follow this pattern:\nRequest Validation - Validate input parameters Authorization - Check user permissions based on role DynamoDB Operations - Query or update data Response Formatting - Return standardized JSON response Error Handling - Catch and return appropriate errors Deployment Deploy all backend services using the Lambda deployment scripts provided in the workshop materials. Each service is packaged with its dependencies and deployed to AWS Lambda with appropriate environment variables.\nTesting Test each service using the provided test scripts or curl commands. Verify:\nCRUD operations work correctly Filtering and search functionality Role-based access control Error handling Performance and response times Next Steps With backend services implemented, proceed to Frontend Development to build the React application.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week Duration: October 20 - 26, 2025\nWeek 7 Objectives: Study AWS RDS and learn about supported database engines. Learn how to launch an RDS instance and configure key settings. Study AWS security services (Cognito, WAF, GuardDuty, etc.). Review AWS service knowledge for midterm exam preparation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Study AWS RDS basics + Explore RDS engines: MySQL, PostgreSQL, MariaDB, Aurora, Oracle, SQL Server + Learn how to launch an RDS instance 10/20/2025 10/20/2025 AWS RDS Docs Tuesday-Wednesday - Study AWS security services + Cognito authentication flow + WAF traffic filtering + GuardDuty threat detection 10/21/2025 10/22/2025 AWS Security Docs Thursday-Friday - Review AWS services for midterm exam + Compute, Storage, IAM, VPC, Networking, Security, Monitoring 10/23/2025 10/24/2025 Lecture Notes Week 7 Achievements: Learned how AWS RDS works and how to deploy a database instance. Understood core AWS security services and their use cases. Completed midterm review covering compute, storage, networking, and security. "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I donâ€™t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/4-eventparticipated/4.3-event3/","title":"Event: CloudThinker: Agentic AI &amp; Orchestration on AWS","tags":[],"description":"","content":"CloudThinker: Agentic AI \u0026amp; Orchestration on AWS Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Friday, December 5, 2025\nEvent Objectives Deep dive into AWS Bedrock Agent Core and its core capabilities Explore real-world use cases for building Agentic Workflows Master advanced concepts like Agentic Orchestration and Context Optimization at deep technical level (L300) Gain hands-on experience through CloudThinker Hack workshop Speaker List Nguyá»…n Gia HÆ°ng Head of Solutions Architect, AWS KiÃªn Nguyá»…n Solutions Architect, AWS Viá»‡t Pháº¡m Founder \u0026amp; CEO, CloudThinker Tháº¯ng TÃ´n Co-founder \u0026amp; COO, CloudThinker Henry BÃ¹i Head of Engineering, CloudThinker Kha VÄƒn Workshop Facilitator Key Content Highlights 1. AWS Foundation and Agentic AI Opening: Nguyen Gia Hung opens the event, emphasizing the increasingly critical role of Agentic AI in today\u0026rsquo;s cloud technology landscape Bedrock Agent Core: Kien Nguyen provides the difficulty of deploying AI agent from developing phase to production phase and how with the help of AWS Bedrock Agent Core, this issue could be solved 2. Real-World Applications Building Agentic Workflows: Viet Pham demonstrates how Diaflow, a data processing platform run on AWS Bedrock Agent Core has leveraged the ability of Agent Core to simplify the process of building AI agent and make it a service available to both personal user and businesses CloudThinker Introduction: Thang Ton introduces CloudThinker ecosystem, the end-to-end platform helps user manaage their cloud resources from cost managing to monitoring usage of different AWS Services 3. Deep Dive (Level 300) Agent Orchestration \u0026amp; Context Optimization: This is the deep technical section of the morning. Henry Bui discusses advanced strategies for orchestrating multi-agent systems and context optimization in Amazon Bedrock, ensuring high accuracy in complex interactions 4. Hands-on Practice CloudThinker Hack: Facilitated by Kha Van, this 60-minute hands-on session allows participants to try out the CloudThinker services with free trial credit, showing the power of CloudThinker in Cloud environment and how it could be leveraged to simplify the management of hundreds of different services available on AWS Key Takeaways Evolution of AI From Chat to Action: The technology field is witnessing a strong shift from chatbots that only respond passively to proactive Agents capable of performing complex orchestration and calling APIs to complete tasks Context is Key: As workflows become increasingly complex, standard context windows are insufficient. Applying \u0026ldquo;Context Optimization\u0026rdquo; strategies is essential for reducing operational costs while increasing Agent accuracy Architecture Orchestration Model: Managing a system with multiple Agents requires a strong Orchestration Layer to allocate and decide which Agent handles which part of user requests Application to Work Prototype Agent: Use AWS Bedrock Agent to build simple internal tools capable of connecting to existing company APIs (for example: checking server status or leave statistics) Research Context Models: Deep dive into context optimization techniques shared by Henry BÃ¹i to apply to current RAG systems for improved performance Join Hackathons: Encourage technical team to participate in similar real hackathons for the fastest and most intuitive updates on AWS new features Event Experience The event delved deep into the landscape of AI agent and its role in the business landscape.\nTechnical Depth: The L300 session on Orchestration was particularly valuable, helping me understand how to scale AI applications beyond simple demos Interactivity: The \u0026ldquo;CloudThinker Hack\u0026rdquo; section helped me immediately reinforce theoretical knowledge with practice, making this one of the most effective learning sessions Some photos from the event Overall, this event provided a clear architectural framework for developing Agentic AI Systems capable of complex orchestration and execution on AWS cloud platform.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.8-frontend/","title":"Frontend Development","tags":[],"description":"","content":"Frontend Development This section covers building the React frontend application for InsightHR.\nOverview The frontend is a Single Page Application (SPA) built with:\nReact 18 with TypeScript Vite 7.2 for fast builds Tailwind CSS 3.4 for styling (Frutiger Aero theme) Zustand 5.0 for state management React Hook Form + Zod for form validation Recharts 3.4 for data visualization React Router v7 for routing Project Structure insighthr-web/ â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ components/ â”‚ â”‚ â”œâ”€â”€ auth/ # Login, Register, ProtectedRoute â”‚ â”‚ â”œâ”€â”€ admin/ # User/Employee/Score Management â”‚ â”‚ â”œâ”€â”€ dashboard/ # Charts, Tables, Filters â”‚ â”‚ â”œâ”€â”€ attendance/ # Check-in/out, History â”‚ â”‚ â”œâ”€â”€ chatbot/ # AI Chat Interface â”‚ â”‚ â”œâ”€â”€ profile/ # User Profile â”‚ â”‚ â”œâ”€â”€ common/ # Reusable UI components â”‚ â”‚ â””â”€â”€ layout/ # Header, Sidebar, Footer â”‚ â”œâ”€â”€ pages/ # Page components â”‚ â”œâ”€â”€ services/ # API service layer â”‚ â”œâ”€â”€ store/ # Zustand state stores â”‚ â”œâ”€â”€ types/ # TypeScript type definitions â”‚ â”œâ”€â”€ utils/ # Utility functions â”‚ â””â”€â”€ styles/ # Global styles and theme â”œâ”€â”€ public/ # Static assets â””â”€â”€ package.json # Dependencies Key Components Authentication Components LoginForm: Email/password and Google OAuth login RegisterForm: New user registration ProtectedRoute: Route guard for authenticated users ChangePasswordModal: Password change dialog Admin Panel Components UserManagement: User CRUD with role assignment EmployeeManagement: Employee CRUD with filters PerformanceScoreManagement: Score management with bulk import PasswordRequestsPanel: Password reset request handling Dashboard Components PerformanceDashboard: Main analytics dashboard BarChart: Department performance comparison LineChart: Trend analysis over time PieChart: Score distribution DataTable: Sortable, filterable data grid FilterPanel: Date range and department filters ExportButton: CSV/Excel export Chatbot Components MessageList: Conversation history display MessageInput: User input with send button TypingIndicator: Loading animation ChatbotInstructions: Usage guide Attendance Components CheckInCheckOut: Quick check-in/out buttons AttendanceManagement: Full attendance interface AttendanceCalendarView: Calendar visualization AttendanceRecordsList: History table State Management Using Zustand for global state:\n// Auth Store interface AuthState { user: User | null; tokens: Tokens | null; isAuthenticated: boolean; login: (email: string, password: string) =\u0026gt; Promise\u0026lt;void\u0026gt;; logout: () =\u0026gt; void; } // Employee Store interface EmployeeState { employees: Employee[]; loading: boolean; error: string | null; fetchEmployees: (filters?: Filters) =\u0026gt; Promise\u0026lt;void\u0026gt;; createEmployee: (data: EmployeeInput) =\u0026gt; Promise\u0026lt;void\u0026gt;; } API Integration All API calls go through a centralized service layer with axios interceptors for authentication:\nconst api = axios.create({ baseURL: import.meta.env.VITE_API_BASE_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); // Add auth token to all requests api.interceptors.request.use((config) =\u0026gt; { const token = localStorage.getItem(\u0026#39;idToken\u0026#39;); if (token) { config.headers.Authorization = `Bearer ${token}`; } return config; }); Routing React Router v7 handles navigation:\nconst router = createBrowserRouter([ { path: \u0026#39;/login\u0026#39;, element: \u0026lt;LoginPage /\u0026gt; }, { path: \u0026#39;/register\u0026#39;, element: \u0026lt;RegisterForm /\u0026gt; }, { path: \u0026#39;/\u0026#39;, element: \u0026lt;ProtectedRoute\u0026gt;\u0026lt;Layout /\u0026gt;\u0026lt;/ProtectedRoute\u0026gt;, children: [ { path: \u0026#39;/\u0026#39;, element: \u0026lt;DashboardPage /\u0026gt; }, { path: \u0026#39;/admin\u0026#39;, element: \u0026lt;AdminPage /\u0026gt; }, { path: \u0026#39;/employees\u0026#39;, element: \u0026lt;EmployeesPage /\u0026gt; }, { path: \u0026#39;/performance-scores\u0026#39;, element: \u0026lt;PerformanceScoresPage /\u0026gt; }, { path: \u0026#39;/attendance\u0026#39;, element: \u0026lt;AttendancePage /\u0026gt; }, { path: \u0026#39;/chatbot\u0026#39;, element: \u0026lt;ChatbotPage /\u0026gt; }, { path: \u0026#39;/profile\u0026#39;, element: \u0026lt;ProfilePage /\u0026gt; } ] } ]); Environment Configuration Create .env file:\nVITE_API_BASE_URL=https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=your-user-pool-id VITE_COGNITO_CLIENT_ID=your-client-id Development # Install dependencies npm install # Start development server npm run dev # Build for production npm run build # Preview production build npm run preview Styling Tailwind CSS with custom Frutiger Aero theme provides a modern, clean interface with:\nGradient backgrounds Glassmorphism effects Smooth animations Responsive design Accessible components Testing The frontend includes:\nComponent unit tests Integration tests for user flows E2E tests with Playwright Accessibility testing Next Steps With the frontend built, proceed to Deployment to deploy the complete application to AWS.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week Duration: October 27 â€“ November 02\nWeek 8 Objectives: Review AWS core services for the midterm exam. Study the pillars of the AWS Well-Architected Framework. Understand how each pillar affects cloud architecture and best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday-Friday - Review AWS services (IAM, EC2, S3, RDS, VPC, Lambda, CloudWatch, CloudTrail) - Study all 6 pillars of AWS Well-Architected Framework: + Operational Excellence + Security + Reliability + Performance Efficiency + Cost Optimization + Sustainability 10/27/2025 10/31/2025 AWS WAF Whitepaper Week 8 Achievements: Completed midterm AWS service review. Understood all Well-Architected Framework pillars and how they guide cloud architecture. Strengthened overall AWS knowledge before entering advanced project stages. "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.9-deployment/","title":"Deployment","tags":[],"description":"","content":"Deploying InsightHR to Production This section covers deploying the complete InsightHR application to AWS, including frontend deployment to S3/CloudFront and backend Lambda functions.\nDeployment Overview The deployment process consists of:\nFrontend Deployment: Build and deploy React app to S3 CloudFront Configuration: Set up CDN for global distribution Lambda Deployment: Deploy all backend functions API Gateway Configuration: Set up REST API endpoints Environment Configuration: Configure environment variables DNS Setup (Optional): Configure custom domain Prerequisites Before deploying, ensure you have:\nâœ… All DynamoDB tables created âœ… Cognito User Pool configured âœ… IAM roles and policies set up âœ… AWS CLI configured âœ… Node.js and npm installed âœ… Python 3.11+ installed Step 1: Frontend Deployment Build the React Application Navigate to the frontend directory and build the production bundle:\ncd insighthr-web # Install dependencies npm install # Build production bundle npm run build This creates an optimized production build in the dist/ directory.\nBuild Output:\ndist/ â”œâ”€â”€ index.html â”œâ”€â”€ assets/ â”‚ â”œâ”€â”€ index-[hash].js â”‚ â”œâ”€â”€ index-[hash].css â”‚ â””â”€â”€ [other assets] â””â”€â”€ [other files] Create S3 Bucket Create an S3 bucket for hosting the static website:\n# Create bucket aws s3 mb s3://insighthr-web-app-sg --region ap-southeast-1 # Enable static website hosting aws s3 website s3://insighthr-web-app-sg \\ --index-document index.html \\ --error-document index.html Configure Bucket Policy Create a bucket policy to allow CloudFront access:\nbucket-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::insighthr-web-app-sg/*\u0026#34; } ] } Apply the policy:\naws s3api put-bucket-policy \\ --bucket insighthr-web-app-sg \\ --policy file://bucket-policy.json Upload Files to S3 # Sync dist folder to S3 aws s3 sync dist/ s3://insighthr-web-app-sg \\ --region ap-southeast-1 \\ --delete # Verify upload aws s3 ls s3://insighthr-web-app-sg --recursive Step 2: CloudFront Configuration Create CloudFront Distribution Using AWS Console:\nNavigate to CloudFront\nClick \u0026ldquo;Create Distribution\u0026rdquo;\nConfigure origin:\nOrigin domain: insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com Origin path: (leave empty) Name: S3-insighthr-web-app Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS Cache policy: CachingOptimized Settings:\nPrice class: Use all edge locations Alternate domain names (CNAMEs): insight-hr.io.vn, www.insight-hr.io.vn Custom SSL certificate: Request or import certificate Default root object: index.html Click \u0026ldquo;Create Distribution\u0026rdquo;\nUsing AWS CLI:\n# Create distribution configuration cat \u0026gt; cloudfront-config.json \u0026lt;\u0026lt; EOF { \u0026#34;CallerReference\u0026#34;: \u0026#34;insighthr-$(date +%s)\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;InsightHR CloudFront Distribution\u0026#34;, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 3, \u0026#34;Items\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: {\u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34;} }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000 }, \u0026#34;Enabled\u0026#34;: true } EOF # Create distribution aws cloudfront create-distribution \\ --distribution-config file://cloudfront-config.json Configure Custom Error Pages Set up error pages for SPA routing:\nGo to CloudFront distribution\nNavigate to \u0026ldquo;Error Pages\u0026rdquo; tab\nCreate custom error response:\nHTTP error code: 403 Customize error response: Yes Response page path: /index.html HTTP response code: 200 Repeat for error code 404\nStep 3: Lambda Deployment Package Lambda Functions For each Lambda function, create a deployment package:\nExample: Auth Login Handler\ncd lambda/auth # Create deployment package mkdir -p package pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy to Lambda aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::ACCOUNT_ID:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=ap-southeast-1_rzDtdAhvp, CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Deploy All Lambda Functions Create a deployment script:\ndeploy-all-lambdas.sh:\n#!/bin/bash FUNCTIONS=( \u0026#34;auth-login-handler\u0026#34; \u0026#34;auth-register-handler\u0026#34; \u0026#34;auth-google-handler\u0026#34; \u0026#34;employees-handler\u0026#34; \u0026#34;employees-bulk-handler\u0026#34; \u0026#34;performance-scores-handler\u0026#34; \u0026#34;chatbot-handler\u0026#34; \u0026#34;attendance-handler\u0026#34; ) for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deploying $func...\u0026#34; # Package and deploy logic here done Step 4: API Gateway Configuration Create REST API # Create API aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 Create Resources and Methods Example: Create /employees endpoint\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text) # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text) # Create /employees resource EMPLOYEES_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part employees \\ --query \u0026#39;id\u0026#39; \\ --output text) # Create GET method aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --authorization-type COGNITO_USER_POOLS \\ --authorizer-id $AUTHORIZER_ID # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:ACCOUNT_ID:function:insighthr-employees-handler/invocations Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; # Get API endpoint echo \u0026#34;API Endpoint: https://$API_ID.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 5: Environment Configuration Frontend Environment Variables Create .env.production file:\nVITE_API_BASE_URL=https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=ap-southeast-1_rzDtdAhvp VITE_COGNITO_CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5 Rebuild and redeploy frontend:\nnpm run build aws s3 sync dist/ s3://insighthr-web-app-sg --delete Lambda Environment Variables Update Lambda functions with environment variables:\naws lambda update-function-configuration \\ --function-name insighthr-employees-handler \\ --environment Variables=\u0026#34;{ AWS_REGION=ap-southeast-1, EMPLOYEES_TABLE=insighthr-employees-dev, USERS_TABLE=insighthr-users-dev }\u0026#34; Step 6: CloudFront Cache Invalidation After deploying frontend changes, invalidate CloudFront cache:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; Step 7: DNS Configuration (Optional) If using a custom domain:\nRequest SSL Certificate # Request certificate in us-east-1 (required for CloudFront) aws acm request-certificate \\ --domain-name insight-hr.io.vn \\ --subject-alternative-names www.insight-hr.io.vn \\ --validation-method DNS \\ --region us-east-1 Configure Route53 # Create hosted zone aws route53 create-hosted-zone \\ --name insight-hr.io.vn \\ --caller-reference $(date +%s) # Create A record for CloudFront cat \u0026gt; change-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id ZONE_ID \\ --change-batch file://change-batch.json Deployment Checklist Before going live, verify:\nAll Lambda functions deployed and tested API Gateway endpoints configured DynamoDB tables populated with initial data Cognito User Pool configured Frontend built and deployed to S3 CloudFront distribution active SSL certificate validated (if using custom domain) DNS records configured (if using custom domain) Environment variables set correctly CORS configured on API Gateway CloudWatch logs enabled IAM roles and permissions verified Testing Deployment Test Frontend # Access CloudFront URL curl -I https://d2z6tht6rq32uy.cloudfront.net # Or custom domain curl -I https://insight-hr.io.vn Test API Endpoints # Test health check curl https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/health # Test authenticated endpoint (with token) curl -H \u0026#34;Authorization: Bearer YOUR_JWT_TOKEN\u0026#34; \\ https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/employees Continuous Deployment For automated deployments, consider:\nGitHub Actions: Automate build and deploy on push AWS CodePipeline: Full CI/CD pipeline AWS Amplify: Simplified frontend deployment Troubleshooting Frontend not loading:\nCheck S3 bucket policy Verify CloudFront distribution status Check browser console for errors API errors:\nVerify Lambda function logs in CloudWatch Check API Gateway configuration Verify Cognito authorizer setup CORS issues:\nConfigure CORS on API Gateway Check allowed origins match frontend domain Next Steps With deployment complete, proceed to Testing \u0026amp; Monitoring to set up CloudWatch monitoring and synthetic canaries.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week Duration: November 3 - 9, 2025\nWeek 9 Objectives: Deploy the auto_scoring ML model into the AWS environment. Learn how Lambda layers work and how to package dependencies. Practice building Docker images and pushing them to Amazon ECR. Deploy the model successfully using ECR-based Lambda. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Deploy auto_scoring model to AWS environment + Test basic inference 11/03/2025 11/03/2025 Project Docs Tuesday-Wednesday - Study Lambda dependency management + Create Lambda layers + Check size limitations + Explore alternative deployment methods 11/04/2025 11/05/2025 AWS Lambda Docs Thursday - Build Docker image for model - Push image to Amazon ECR - Configure IAM permissions 11/06/2025 11/06/2025 AWS ECR Docs Friday - Deploy Lambda using ECR container - Run model inference tests 11/07/2025 11/07/2025 AWS Lambda + ECR Docs Week 9 Achievements: Learned the file-size limitations of Lambda layers. Successfully packaged the ML model using a Docker image. Uploaded the image to Amazon ECR and deployed it to Lambda. Fully deployed the auto_scoring model into AWS with container-based execution. Understood how ECR is essential for large machine learning models with many dependencies. "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.10-testing/","title":"Testing &amp; Monitoring","tags":[],"description":"","content":"Testing \u0026amp; Monitoring This section covers setting up comprehensive testing and monitoring for the InsightHR platform.\nOverview We\u0026rsquo;ll implement:\nCloudWatch Logs - Centralized logging CloudWatch Metrics - Performance monitoring CloudWatch Alarms - Automated alerting CloudWatch Synthetics - Automated testing X-Ray Tracing - Distributed tracing (optional) CloudWatch Logs All Lambda functions automatically log to CloudWatch Logs.\nView Logs:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Tail logs for a function aws logs tail /aws/lambda/insighthr-auth-login-handler \\ --follow \\ --region ap-southeast-1 Log Insights Queries:\nFind errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20 Analyze response times:\nfields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) CloudWatch Metrics Monitor key metrics:\nLambda Metrics:\nInvocations Duration Errors Throttles Concurrent executions DynamoDB Metrics:\nRead/Write capacity units Throttled requests System errors API Gateway Metrics:\nRequest count Latency 4XX/5XX errors View Metrics:\n# Lambda invocations aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --start-time 2025-12-09T00:00:00Z \\ --end-time 2025-12-09T23:59:59Z \\ --period 3600 \\ --statistics Sum \\ --region ap-southeast-1 CloudWatch Alarms Create alarms for critical metrics:\nHigh Error Rate Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-error-rate \\ --alarm-description \u0026#34;Alert when Lambda error rate exceeds 5%\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --region ap-southeast-1 High Latency Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-latency \\ --alarm-description \u0026#34;Alert when API latency exceeds 2 seconds\u0026#34; \\ --metric-name Latency \\ --namespace AWS/ApiGateway \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 2000 \\ --comparison-operator GreaterThanThreshold \\ --region ap-southeast-1 CloudWatch Synthetics Canaries Automated testing with synthetic canaries:\n1. Login Canary - Tests login flow 2. Dashboard Canary - Tests dashboard loading 3. Chatbot Canary - Tests chatbot queries 4. Autoscoring Canary - Tests performance calculations\nCreate Login Canary:\n// login-canary.js const synthetics = require(\u0026#39;Synthetics\u0026#39;); const log = require(\u0026#39;SyntheticsLogger\u0026#39;); const loginFlow = async function () { const page = await synthetics.getPage(); // Navigate to login page await page.goto(\u0026#39;https://insight-hr.io.vn/login\u0026#39;, { waitUntil: \u0026#39;domcontentloaded\u0026#39;, timeout: 30000 }); // Fill login form await page.type(\u0026#39;#email\u0026#39;, \u0026#39;test@example.com\u0026#39;); await page.type(\u0026#39;#password\u0026#39;, \u0026#39;Test123!\u0026#39;); // Click login button await Promise.all([ page.waitForNavigation(), page.click(\u0026#39;button[type=\u0026#34;submit\u0026#34;]\u0026#39;) ]); // Verify successful login await page.waitForSelector(\u0026#39;.dashboard\u0026#39;, { timeout: 10000 }); log.info(\u0026#39;Login flow completed successfully\u0026#39;); }; exports.handler = async () =\u0026gt; { return await synthetics.executeStep(\u0026#39;LoginFlow\u0026#39;, loginFlow); }; Deploy Canary:\n# Create S3 bucket for canary artifacts aws s3 mb s3://insighthr-canary-artifacts --region ap-southeast-1 # Create canary aws synthetics create-canary \\ --name insighthr-login-canary \\ --artifact-s3-location s3://insighthr-canary-artifacts \\ --execution-role-arn arn:aws:iam::${ACCOUNT_ID}:role/CloudWatchSyntheticsRole \\ --schedule Expression=\u0026#34;rate(5 minutes)\u0026#34; \\ --runtime-version syn-nodejs-puppeteer-6.2 \\ --code file://login-canary.zip \\ --region ap-southeast-1 Dashboard Setup Create a CloudWatch Dashboard:\naws cloudwatch put-dashboard \\ --dashboard-name InsightHR-Dashboard \\ --dashboard-body file://dashboard-config.json \\ --region ap-southeast-1 dashboard-config.json:\n{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApiGateway\u0026#34;, \u0026#34;Count\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;4XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;5XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Gateway Metrics\u0026#34; } } ] } Testing Strategy Unit Tests:\nTest individual Lambda functions Mock DynamoDB and Cognito calls Verify business logic Integration Tests:\nTest API endpoints end-to-end Verify data flow through services Test authentication and authorization Load Tests:\nUse Artillery or k6 for load testing Test concurrent user scenarios Identify performance bottlenecks Example Load Test:\n# load-test.yml config: target: \u0026#39;https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#39; phases: - duration: 60 arrivalRate: 10 scenarios: - name: \u0026#34;Login and fetch employees\u0026#34; flow: - post: url: \u0026#34;/auth/login\u0026#34; json: email: \u0026#34;test@example.com\u0026#34; password: \u0026#34;Test123!\u0026#34; - get: url: \u0026#34;/employees\u0026#34; Best Practices âœ… Structured Logging: Use JSON format for logs âœ… Correlation IDs: Track requests across services âœ… Error Tracking: Log errors with context âœ… Performance Monitoring: Track response times âœ… Automated Testing: Run canaries regularly âœ… Alerting: Set up alarms for critical metrics âœ… Cost Monitoring: Track AWS costs\nTroubleshooting High Error Rates:\nCheck CloudWatch Logs for error details Verify IAM permissions Check DynamoDB capacity Review recent code changes High Latency:\nOptimize DynamoDB queries Add caching where appropriate Review Lambda memory allocation Check cold start times Failed Canaries:\nReview canary logs Check application availability Verify test credentials Update canary scripts if UI changed Monitoring Checklist CloudWatch Logs configured for all Lambda functions Key metrics tracked (invocations, errors, duration) Alarms set up for critical thresholds Synthetics canaries deployed and running Dashboard created for visualization Log retention policies configured Cost alerts configured On-call rotation established (for production) Next Steps With monitoring in place, proceed to Cleanup when you\u0026rsquo;re done with the workshop to avoid ongoing charges.\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week Duration: November 10 - 16, 2025\nWeek 10 Objectives: Study Retrieval-Augmented Generation (RAG) and its applications. Learn how RAG improves model performance using business-specific data. Deploy a RAG system in a local environment. Use LangChain and Bedrock API to build a complete RAG pipeline. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Study fundamentals of RAG + Use cases + Architecture + Vector databases 11/10/2025 11/10/2025 RAG Docs Tuesday-Thursday - Deploy RAG in local environment + Build LangChain pipeline + Test embeddings and retrievers + Integrate Bedrock API 11/11/2025 11/13/2025 LangChain + Bedrock Docs Friday - Successfully run full RAG system locally - Validate retrieval and responses 11/14/2025 11/14/2025 Project Notes Saturday - Paricipating in AWS Mastery 1: AWS AI/ML \u0026amp; GenAI Workshop 11/14/2025 11/14/2025 Project Notes Week 10 Achievements: Understood how RAG helps organizations leverage their internal data. Learned to use LangChain for RAG and LLM-related workflows. Successfully used Bedrock API for embeddings and LLM inference. Built and executed a complete RAG system in a local environment. Learning fundamental about GenAI and how to build one using AWS services "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/5-workshop/5.11-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Cleaning Up Resources To avoid ongoing charges, it\u0026rsquo;s important to delete all AWS resources created during this workshop. This section provides step-by-step instructions for cleaning up.\nImportant: Deleting resources is irreversible. Make sure you\u0026rsquo;ve backed up any data you want to keep before proceeding.\nCleanup Overview We\u0026rsquo;ll delete resources in the following order:\nCloudFront Distribution S3 Buckets API Gateway Lambda Functions DynamoDB Tables Cognito User Pool CloudWatch Logs IAM Roles and Policies Route53 (if configured) ACM Certificates (if created) Step 1: Delete CloudFront Distribution CloudFront distributions must be disabled before deletion.\nUsing AWS Console:\nNavigate to CloudFront Select the InsightHR distribution Click \u0026ldquo;Disable\u0026rdquo; Wait for status to change to \u0026ldquo;Disabled\u0026rdquo; (may take 15-20 minutes) Select the distribution again Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Get ETag ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) # Disable distribution aws cloudfront get-distribution-config --id $DIST_ID \u0026gt; dist-config.json # Edit dist-config.json: Set \u0026#34;Enabled\u0026#34;: false aws cloudfront update-distribution \\ --id $DIST_ID \\ --if-match $ETAG \\ --distribution-config file://dist-config.json # Wait for distribution to be disabled aws cloudfront wait distribution-deployed --id $DIST_ID # Delete distribution ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) aws cloudfront delete-distribution --id $DIST_ID --if-match $ETAG Step 2: Delete S3 Buckets Empty and delete the web app bucket:\n# Empty bucket aws s3 rm s3://insighthr-web-app-sg --recursive # Delete bucket aws s3 rb s3://insighthr-web-app-sg --force Delete canary artifacts bucket (if created):\naws s3 rm s3://insighthr-canary-artifacts --recursive aws s3 rb s3://insighthr-canary-artifacts --force Step 3: Delete API Gateway Using AWS Console:\nNavigate to API Gateway Select \u0026ldquo;InsightHR API\u0026rdquo; Click \u0026ldquo;Actions\u0026rdquo; â†’ \u0026ldquo;Delete API\u0026rdquo; Confirm deletion Using AWS CLI:\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete API aws apigateway delete-rest-api \\ --rest-api-id $API_ID \\ --region ap-southeast-1 Step 4: Delete Lambda Functions Delete all Lambda functions:\n# List of functions to delete FUNCTIONS=( \u0026#34;insighthr-auth-login-handler\u0026#34; \u0026#34;insighthr-auth-register-handler\u0026#34; \u0026#34;insighthr-auth-google-handler\u0026#34; \u0026#34;insighthr-employees-handler\u0026#34; \u0026#34;insighthr-employees-bulk-handler\u0026#34; \u0026#34;insighthr-performance-scores-handler\u0026#34; \u0026#34;insighthr-chatbot-handler\u0026#34; \u0026#34;insighthr-attendance-handler\u0026#34; ) # Delete each function for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deleting $func...\u0026#34; aws lambda delete-function \\ --function-name $func \\ --region ap-southeast-1 done Step 5: Delete DynamoDB Tables Using AWS Console:\nNavigate to DynamoDB Select each table Click \u0026ldquo;Delete\u0026rdquo; Confirm by typing \u0026ldquo;delete\u0026rdquo; Using AWS CLI:\n# List of tables to delete TABLES=( \u0026#34;insighthr-users-dev\u0026#34; \u0026#34;insighthr-employees-dev\u0026#34; \u0026#34;insighthr-performance-scores-dev\u0026#34; \u0026#34;insighthr-attendance-history-dev\u0026#34; \u0026#34;insighthr-password-reset-requests-dev\u0026#34; \u0026#34;insighthr-kpis-dev\u0026#34; \u0026#34;insighthr-formulas-dev\u0026#34; \u0026#34;insighthr-data-tables-dev\u0026#34; \u0026#34;insighthr-notification-rules-dev\u0026#34; ) # Delete each table for table in \u0026#34;${TABLES[@]}\u0026#34;; do echo \u0026#34;Deleting $table...\u0026#34; aws dynamodb delete-table \\ --table-name $table \\ --region ap-southeast-1 done Step 6: Delete Cognito User Pool Using AWS Console:\nNavigate to Cognito Select \u0026ldquo;User Pools\u0026rdquo; Select the InsightHR user pool Click \u0026ldquo;Delete user pool\u0026rdquo; Type the pool name to confirm Using AWS CLI:\n# Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete user pool aws cognito-idp delete-user-pool \\ --user-pool-id $USER_POOL_ID \\ --region ap-southeast-1 Step 7: Delete CloudWatch Logs Delete log groups:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Delete each log group LOG_GROUPS=$(aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --query \u0026#39;logGroups[*].logGroupName\u0026#39; \\ --output text \\ --region ap-southeast-1) for log_group in $LOG_GROUPS; do echo \u0026#34;Deleting $log_group...\u0026#34; aws logs delete-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 done Delete CloudWatch Synthetics Canaries (if created):\n# List canaries aws synthetics describe-canaries \\ --region ap-southeast-1 # Delete each canary CANARIES=$(aws synthetics describe-canaries \\ --query \u0026#39;Canaries[?starts_with(Name, `insighthr`)].Name\u0026#39; \\ --output text \\ --region ap-southeast-1) for canary in $CANARIES; do echo \u0026#34;Deleting canary $canary...\u0026#34; aws synthetics delete-canary \\ --name $canary \\ --region ap-southeast-1 done Step 8: Delete IAM Roles and Policies Using AWS Console:\nNavigate to IAM Go to \u0026ldquo;Roles\u0026rdquo; Search for \u0026ldquo;insighthr\u0026rdquo; Select each role Detach policies Delete role Using AWS CLI:\n# List roles aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text # For each role, detach policies and delete ROLES=$(aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text) for role in $ROLES; do echo \u0026#34;Processing role $role...\u0026#34; # Detach managed policies POLICIES=$(aws iam list-attached-role-policies \\ --role-name $role \\ --query \u0026#39;AttachedPolicies[*].PolicyArn\u0026#39; \\ --output text) for policy in $POLICIES; do aws iam detach-role-policy \\ --role-name $role \\ --policy-arn $policy done # Delete inline policies INLINE_POLICIES=$(aws iam list-role-policies \\ --role-name $role \\ --query \u0026#39;PolicyNames[*]\u0026#39; \\ --output text) for policy in $INLINE_POLICIES; do aws iam delete-role-policy \\ --role-name $role \\ --policy-name $policy done # Delete role aws iam delete-role --role-name $role done Delete custom policies:\n# List custom policies aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text # Delete each policy POLICIES=$(aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text) for policy in $POLICIES; do echo \u0026#34;Deleting policy $policy...\u0026#34; # Delete all policy versions except default VERSIONS=$(aws iam list-policy-versions \\ --policy-arn $policy \\ --query \u0026#39;Versions[?!IsDefaultVersion].VersionId\u0026#39; \\ --output text) for version in $VERSIONS; do aws iam delete-policy-version \\ --policy-arn $policy \\ --version-id $version done # Delete policy aws iam delete-policy --policy-arn $policy done Step 9: Delete Route53 Resources (If Configured) Delete DNS records:\n# Get hosted zone ID ZONE_ID=$(aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;insight-hr.io.vn.\u0026#39;].Id\u0026#34; \\ --output text) # List and delete records (except NS and SOA) aws route53 list-resource-record-sets \\ --hosted-zone-id $ZONE_ID # Delete A records for CloudFront cat \u0026gt; delete-records.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id $ZONE_ID \\ --change-batch file://delete-records.json # Delete hosted zone aws route53 delete-hosted-zone --id $ZONE_ID Step 10: Delete ACM Certificates (If Created) Using AWS Console:\nNavigate to Certificate Manager (in us-east-1 region) Select the certificate Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# List certificates aws acm list-certificates \\ --region us-east-1 # Delete certificate CERT_ARN=$(aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;insight-hr.io.vn\u0026#39;].CertificateArn\u0026#34; \\ --output text \\ --region us-east-1) aws acm delete-certificate \\ --certificate-arn $CERT_ARN \\ --region us-east-1 Automated Cleanup Script Create a comprehensive cleanup script:\ncleanup-all.sh:\n#!/bin/bash set -e echo \u0026#34;Starting InsightHR cleanup...\u0026#34; # 1. Disable and delete CloudFront echo \u0026#34;Step 1: CloudFront...\u0026#34; # (Add CloudFront cleanup commands) # 2. Delete S3 buckets echo \u0026#34;Step 2: S3 Buckets...\u0026#34; aws s3 rm s3://insighthr-web-app-sg --recursive aws s3 rb s3://insighthr-web-app-sg --force # 3. Delete API Gateway echo \u0026#34;Step 3: API Gateway...\u0026#34; # (Add API Gateway cleanup commands) # 4. Delete Lambda functions echo \u0026#34;Step 4: Lambda Functions...\u0026#34; # (Add Lambda cleanup commands) # 5. Delete DynamoDB tables echo \u0026#34;Step 5: DynamoDB Tables...\u0026#34; # (Add DynamoDB cleanup commands) # 6. Delete Cognito echo \u0026#34;Step 6: Cognito User Pool...\u0026#34; # (Add Cognito cleanup commands) # 7. Delete CloudWatch logs echo \u0026#34;Step 7: CloudWatch Logs...\u0026#34; # (Add CloudWatch cleanup commands) # 8. Delete IAM roles echo \u0026#34;Step 8: IAM Roles and Policies...\u0026#34; # (Add IAM cleanup commands) echo \u0026#34;Cleanup complete!\u0026#34; Verification After cleanup, verify all resources are deleted:\n# Check S3 buckets aws s3 ls | grep insighthr # Check Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?starts_with(FunctionName, `insighthr`)].FunctionName\u0026#39; \\ --region ap-southeast-1 # Check DynamoDB tables aws dynamodb list-tables \\ --query \u0026#39;TableNames[?starts_with(@, `insighthr`)]\u0026#39; \\ --region ap-southeast-1 # Check CloudFront distributions aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; # Check API Gateway aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --region ap-southeast-1 Cost Verification After cleanup, monitor your AWS billing:\nGo to AWS Billing Dashboard Check \u0026ldquo;Bills\u0026rdquo; for current month Verify no ongoing charges for deleted services Check \u0026ldquo;Cost Explorer\u0026rdquo; for trends Cleanup Checklist CloudFront distribution disabled and deleted S3 buckets emptied and deleted API Gateway deleted All Lambda functions deleted All DynamoDB tables deleted Cognito User Pool deleted CloudWatch log groups deleted CloudWatch Synthetics canaries deleted IAM roles and policies deleted Route53 hosted zone deleted (if created) ACM certificates deleted (if created) Billing dashboard checked for ongoing charges Troubleshooting Cleanup CloudFront won\u0026rsquo;t delete:\nEnsure distribution is fully disabled Wait 15-20 minutes after disabling Check for associated resources S3 bucket won\u0026rsquo;t delete:\nEnsure bucket is completely empty Check for versioned objects Disable versioning before deleting IAM role won\u0026rsquo;t delete:\nDetach all managed policies first Delete all inline policies Check for service-linked roles DynamoDB table deletion fails:\nWait for table to be in ACTIVE state Check for ongoing operations Verify IAM permissions Final Notes Best Practice: Always clean up resources after completing a workshop to avoid unexpected charges. Set up billing alerts to notify you of any ongoing costs.\nCongratulations! You\u0026rsquo;ve successfully completed the InsightHR workshop and cleaned up all resources. You\u0026rsquo;ve learned:\nâœ… Serverless architecture design âœ… AWS Lambda and API Gateway âœ… DynamoDB data modeling âœ… Cognito authentication âœ… AWS Bedrock AI integration âœ… CloudFront CDN deployment âœ… Infrastructure as Code principles âœ… Cost optimization strategies Thank you for participating in this workshop!\n"},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week Duration: November 17 - 23, 2025\nWeek 11 Objectives: Attempt to deploy the RAG system into the AWS environment. Explore multiple deployment strategies (Lambda layers, ECR container). Research Bedrock Knowledge Base features and pricing. Evaluate whether Bedrock Knowledge is a better alternative to manual RAG deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Participating in AWS Mastery 2: AWS DevOps \u0026amp; Modern Operations 11/17/2025 11/17/2025 Project Docs Tuesday - Attempt deployment of RAG system to AWS environment + Initial Lambda setup 11/18/2025 11/18/2025 Project Docs Wednesday-Thursday - Try multiple deployment methods + Lambda layer (dependency test) + ECR container build and deploy + Debug dependency failures 11/19/2025 11/20/2025 AWS Lambda + ECR Docs Friday - Research Amazon Bedrock Knowledge Base - Study pricing and architecture - Compare with custom RAG deployment 11/21/2025 11/21/2025 Bedrock Docs Week 11 Achievements: Understood the deployment challenges caused by heavy dependencies in RAG pipelines. Realized Lambda layers and ECR containers introduce complexity and size limitations. Shifted strategy to using Bedrock Knowledge Base instead of manually deploying RAG. Studied Knowledge Base pricing and learned how it simplifies ingestion and retrieval. Gain fundamental knowledge of DevOp, CI/CD, etc and how AWS services like ECR, ECS could support those tasks "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week Duration: November 24 - 30, 2025\nWeek 12 Objectives: Learn and deploy AWS Bedrock Knowledge Base. Configure ingestion, embeddings, and retrieval. Find and prepare documents to upload to the Knowledge Base. Test the Knowledge Base performance for RAG-related tasks. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday-Tuesday - Study and deploy Bedrock Knowledge Base + Configure vector store + Set up ingestion 11/24/2025 11/25/2025 Bedrock Docs Wednesday-Friday - Collect documents for ingestion - Upload and test Knowledge Base performance 11/26/2025 11/28/2025 Project Notes Saturday - Participating in AWS Mastery 3: AWS Security Specialty Workshop 11/29/2025 11/29/2025 Week 12 Achievements: Successfully launched RAG system using Bedrock Knowledge Base with minimal setup. Avoided dependency and file-size limitations from manual RAG deployments. Learned how the S3 Vector Engine reduces cost compared to running a PostgreSQL/RDS instance. Confirmed that Bedrock Knowledge Base simplifies and accelerates RAG development. Study fundamental components of security and how AWS services could help in securing it "},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienquang2265.github.io/ThienQuang_worklog.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]